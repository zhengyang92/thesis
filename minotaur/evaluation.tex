\section{Evaluation}
\label{sec:evaluation}

Our primary evaluation metric for \minotaur{} is its ability to speed up
legacy application code, compared to an optimized build using LLVM~18.
%
Secondarily, we look at \minotaur's impact on compile time, optimizations
that have been integrated into upstream LLVM based on our work, and
other issues.


\subsection{Correctness}

Every optimization discovered by \minotaur{} has been formally verified by
Alive2.
%
Even so, bugs might remain in the instruction semantics that we have
added to Alive2, in our cut extractor, in our rewrite mechanism, in
Alive2, or in Z3\@.
%
To defend against implementation errors, we have compiled numerous
open source applications using \minotaur, and then run those applications'
test suites, to ensure that they were not miscompiled.
%
Furthermore, we have compiled SPEC CPU 2017 using \minotaur{} and
used the SPEC drivers to ensure that all of its benchmarks behave
as expected.


\subsection{Effect of Depth Bounds in the Cut Extractor}
\label{sec:loops}

%1345 loops are integer only + vectorizable
%plot only shows 879 loops, these are the loops touched by minotaur.

% 2386 loops are integer / fp + vectorizable
% depthlimit 5: 26339 exprs (305 opt), 290 source changed, 667 min elapsed
% depthlimit 4; 24334 exprs

% \begin{figure*}[tbp]
%   \centering
%   \subfloat[Targeting Intel Cascade Lake; geomean=1.061x\label{plot:loops-intel}]{
%     \includegraphics[page=1,width=\linewidth]{figures/data.pdf}
%   }`
%   \hfill'
%   \subfloat[Targeting AMD Zen 3; geomean=1.021x\label{plot:loops-amd}]{
%     \includegraphics[page=2,width=\linewidth]{figures/data.pdf}
%   }
%   \caption{Speedups---estimated by LLVM-MCA---due to running \tool{}
%     on a loop micro-benchmark suite}
% \end{figure*}

It is important for \minotaur{} to extract cuts that are of an appropriate
size.
%
If they are too large, compile times suffer and also the SMT solver
can be overwhelmed, leading to timeouts; if cuts are too small, then
they form an insufficient basis for driving an optimization.
%
To determine a good value for $B$, the depth parameter to the cut
extraction procedure shown in Algorithm~\ref{alg:slicing}, we
performed an empirical study.
%
We started with FlexC's benchmark suite~\cite{woodruff2023rewriting},
a collection of 2,386 compilable, non-trivial C functions containing
loops from FFMPEG, FreeImage, DarkNet, xz, bzip2, and the LivermoreC
benchmark.
% \footnote{The loop data set was provided
% by Alexander Brauckmann and Michael O'Boyle at the University of
% Edinburgh, UK\@.  At present, no citable reference for this work
% exists.}
%
When compiled to LLVM IR, these functions contain a total of 123,062
instructions; thus, our cut extractor was invoked 123,062 times for
each depth bound.
%
We chose this code as the basis for our experiment because it is
derived from real applications while also being small enough to
keep compile times manageable (compared to, e.g., SPEC CPU 2017,
which is much larger).


\begin{figure}[tbp]
  \centering
  \subfloat[Unique cuts extracted\label{fig:loop-expression}]{
    \includegraphics[width=0.32\linewidth]{figures/cuts.pdf}
  }
  \hfill
  \subfloat[Unique opts. synthesized\label{fig:loop-optimization}]{
    \includegraphics[width=0.32\linewidth]{figures/synthesized.pdf}
  }
  \hfill
  \subfloat[Compilation time\label{fig:loop-buildtime}]{
    \includegraphics[width=0.32\linewidth]{figures/comptime.pdf}
  }
  \caption{Evaluating the effect of varying $B$, the depth bound for
    cut extraction}
  \label{fig:loop}
\end{figure}


We then ran \minotaur{} on these functions with all depth bounds from
0--7, measuring the number of unique cuts that were extracted, the
number of optimizations found, and the compilation time.
%
We used a one-minute timeout for individual Z3 queries, and we also
gave \minotaur{} a total of up to five minutes to synthesize an optimized
version of each cut.
%
Figure~\ref{fig:loop} summarizes the results of this experiment.
%
The number of unique cuts that are extracted grows quickly with $B$,
but eventually begins to saturate simply because the functions being
compiled do not always have very long dependency chains.
%
The number of synthesized optimizations also grows quickly, but it
peaks when $B=6$ and then it decreases because the size of the cuts
causes many solver timeouts.
%
Finally, the total compile time increases smoothly with the depth
bound, eventually leveling off as most solver queries time out.


For the experiments in the rest of the evaluation section, we chose
$B=4$ because this gets close to the maximum observed number of
optimizations without requiring exorbitant compile times.
%
It seems likely that there is room for improvement in this aspect of
\minotaur: perhaps the depth bound should be determined adaptively.
%
In this scenario, we would extract more and more components into the
cut, until either an optimization is found or else the solver begins
to time out.
%
We leave explorations of this nature for future work.


\subsection{What Kind of Optimizations Matter Most?}

To determine which of \minotaur's optimizations matter most, we performed
an ablation study, again using the FlexC benchmark suite that we
described in Section~\ref{sec:loops}.
%
We split the optimizations that \minotaur{} found into five
categories: memory, floating-point vector, floating-point scalar,
integer vector, and integer scalar.
%
Then, we ran \minotaur{} in a way that omitted each of these categories of
optimizations.
%
As shown in Table~\ref{tab:ablation}, integer vector optimizations
produce the most rewrites, and also produce the majority of the
observed speedup.


\subsection{Speedups for Benchmarks and Applications}

In this section, we show how \minotaur{} speeds up real-world benchmarks
and applications.

\paragraph{Experimental setup}
%
We used two machines for our evaluation.
%
The first has an Intel Xeon Gold 6210U processor running at 2.5\,GHz,
and has 20 cores; this implements the Cascade Lake
microarchitecture~\cite{cascadelake} and supports the AVX-512
instruction set.
%
The second has an AMD Ryzen 5950X processor running at 3.4\,GHz, and
has 16 cores; this processor implements the Zen~3
microarchitecture~\cite{zen3}.
%
Both machines run Linux and were idle except for a single core running
our benchmarks (however, when measuring compile times, as reported in
Table~\ref{tab:compiletime}, we used all cores).
%
To reduce the performance variation caused by frequency scaling, we
disabled turbo boost on the Intel machine and core performance
boost on the AMD machine.
%
We also disabled simultaneous multithreading on both machines.


We invoked LLVM with the \texttt{-march=native} compilation flag to
ask it to take maximum advantage of processor features; we left other
compilation flags unchanged, except where noted.
%
All benchmarks are compiled at the \texttt{-O3} optimization level.
%
We set the timeout for Z3~\cite{z3} queries to one minute.
%
Finally, for each instruction that it tries to optimize, \minotaur{} gives
up if no solution is found within five minutes.


\paragraph{Benchmark selection}
%
We evaluate on SPEC CPU 2017\footnote{\url{https://www.spec.org/cpu2017/}}
because it is a widely accepted standard
benchmark.
%
We only evaluate on the \emph{speed} subset of the SPEC suite, and we omit
648.exchange, 607.cactuBSSN, 621.wrf, 627.cam4, 628.pop2, 649.fotonik3d,
and 654.roms as they contain Fortran code.
%
We additionally use GMP, the GNU Multiple Precision
Library,\footnote{\url{https://gmplib.org/}} and
libYUV,\footnote{\url{https://chromium.googlesource.com/libyuv/libyuv/}}
which is used by Google Chrome/Chromium for manipulating images in the
YUV format.
%
We chose these libraries because they have been heavily tuned for
performance, they are loop-intensive, and they come with performance
benchmark suites that we could simply reuse.


\paragraph{Compile times}
%
Table~\ref{tab:compiletime} shows how long it takes \minotaur{} to build
our benchmarks, along with the number of potentially optimizable
values and the number of optimizations found.
%
The compile times are for parallel builds; we set the \textsc{make}'s
\texttt{-j} flag and SPEC CPU 2017's \texttt{build\_ncpus}
configuration to the number of cores on the machine.
%
Minotaur is very slow when it runs with a cold cache because it
performs many solver queries.
%
However, with a warm cache, it is only 3\% slower than baseline \texttt{clang}.


In most cases, \minotaur{} found more optimizations when targeting the AMD
processor.
%
We believe this is because LLVM is more mature targeting
AVX2 than AVX-512.
%
Queries with 256-bit vectors are also less likely to timeout in Z3 than
are queries with 512-bit vectors.

\begin{table}[t]
  \centering
  \caption{Results of an ablation study based on optimization categories}
  \begin{tabular}{ r | r r r r r r}
    & memory & FP vector & FP scalar& int vector & int scalar & overall \\
    \hline
    Rewrites & 3 & 17 & 4 & 191 & 109 & 324 \\
    Geomean & 1.0605x & 1.0600x & 1.0572x & 1.0142x & 1.0506x & 1.0610x \\
    Contribution & 0.65\% & 1.64\% & 5.90\% & 75.57\% & 16.23\% & 100\% \\
  \end{tabular}
  \label{tab:ablation}
\end{table}


\begin{table}[t]
  \centering
  \caption{\minotaur's effect on compilation time on Intel Cascade Lake}
  \begin{tabular}{r | r r r | r r }
    %\cline{2-11}
    & \multicolumn{3}{c|}{Compile time (minutes)} & \multicolumn{2}{c}{Stats} \\
    %\hline
    Benchmarks & Cold & Warm & Clang & \# Cuts & \# Optimizations \\
    \hline
    SPEC CPU 2017 & 2,337 & 3 & 3 & 109,177 & 2,683  \\
    %\hline
    gmp-6.2.1 & 440 &  1 &  1 & 9,170 & 336 \\
    %\hline
    libYUV & 2,196 &  1 &  1 & 6,849 & 334  \\
  \end{tabular}
  \label{tab:compiletime}
\end{table}

\begin{table}[t]
  \centering
  \caption{\minotaur's effect on compilation time on AMD Zen 3}
  \begin{tabular}{r | r r r | r r}

    %\cline{2-11}
    & \multicolumn{3}{c|}{Compile time (minutes)} & \multicolumn{2}{c}{Stats} \\
    %\hline
    Benchmarks & Cold & Warm & Clang & \# Cuts & \# Optimizations \\
    \hline
    SPEC CPU 2017 & 2,580 & 3 & 3 & 114,612 & 2,820 \\
    %\hline
    gmp-6.2.1 &  445 & 1 & 1 & 9,265 & 387\\
    %\hline
    libYUV  & 2,193 & 1 & 1 & 6,809 & 357 \\

  \end{tabular}
  \label{tab:compiletime}
\end{table}


\paragraph{Optimizing GMP with \minotaur{}}

\begin{figure}[tbp]
  \centering
  \subfloat[Speedups on Intel Cascade Lake, geomean = 1.073x\label{plot:gmp-intel}]{
    \includegraphics[width=\linewidth]{figures/gmp-intel.pdf}
  }
  \hfill
  \subfloat[Speedups on AMD Zen 3, geomean = 1.065x\label{plot:gmp-amd}]{
    \includegraphics[width=\linewidth]{figures/gmp-amd.pdf}
  }
  \caption{GNU Multiple Precision Library (GMP) speedups, on a logarithmic scale}
  \label{fig:gmp}
\end{figure}


GMP provides a portable C-language implementation and then, for
several platforms, a faster assembly language implementation.
%
For this evaluation, we selected the C implementation, because \minotaur{}
works on LLVM IR and cannot process assembly code at all.
%
The benchmark suite that we used is
GMPbench.\footnote{\url{https://gmplib.org/gmpbench}}
%
Figure~\ref{fig:gmp} summarizes the results.
%
When \minotaur{} targets the Intel Cascade Lake processor, and when the
resulting executables are run on that same microarchitecture,
all the benchmarks sped up;
across all of the benchmarks, the mean speedup was 7.3\%.
%
The analogous experiment using the AMD Zen~3 microarchitecture
resulted in one benchmark slowing down, and the rest of benchmarks
speeding up, for an overall mean speedup of 6.5\%.


\paragraph{Optimizing libYUV with \minotaur{}}


\begin{figure}[tbp]
  \centering
  \subfloat[Speedups on Intel Cascade Lake, geomean = 1.022x\label{plot:libyuv-intel}]{
    \includegraphics[width=\linewidth]{figures/yuv-intel.pdf}
  }
  \hfill
  \subfloat[Speedups on AMD Zen 3, geomean = 1.029x\label{plot:libyuv-amd}]{
    \includegraphics[width=\linewidth]{figures/yuv-amd.pdf}
  }
  \caption{LibYUV speedups, on a logarithmic scale}
  \label{fig:yuv}
\end{figure}


This library has an extensive test suite, part of which is explicitly
intended for performance testing; we used this part as a benchmark.
%
Each test program scales, rotates, or converts a 1280\,x\,728 pixel
image 1,000 times.
%
Figure~\ref{fig:yuv} shows the results of this experiment.
%
When \minotaur{} targets an Intel processor, $148$ programs slowed down, $72$
did not change performance, and $2,312$ sped up, for an overall speedup of
2.2\%.
%
Targeting an AMD processor, $188$ programs slowed down, $85$ did not
change performance, and $2,259$ sped up, for an overall speedup of 2.9\%.
%
\minotaur{} can make code slower because it looks at optimizations in
isolation; it does not attempt to model interactions between
optimizations.


libYUV is portable code, but it has already been heavily tuned for
performance; most commits to its repository over the last several
years have been performance-related.
%
Our hypothesis is that this manual tuning has already eaten up most of
the performance gains that we would have hoped to gain from \minotaur{}.
%
For some time now, Google's released versions of Chrome have been
compiled using LLVM; the Chrome engineers have had ample time to
ensure that this compiler achieves decent code generation for
performance-critical libraries.

% \begin{figure*}[tbp]
%   \centering
%   \subfloat[Normalized speedup; geomean = 1.013x\label{fig:spec-intel-speed-ups}]{
%     \includegraphics[width=0.45\linewidth]{figures/spec/spec-intel.pdf}
%   }
%   \hfill
%   \subfloat[Compilation time in seconds\label{fig:spec-intel-compilation-time}]{
%     \includegraphics[width=0.45\linewidth]{figures/spec/spec-intel-compiletime.pdf}
%   }
%   \caption*{Targeting Intel Cascade Lake\label{fig:spec-intel}}
%   \hfill
%   \subfloat[Normalized speedup; geomean = 1.012x\label{fig:spec-amd-speed-ups}]{
%     \includegraphics[width=0.45\linewidth]{figures/spec/spec-amd.pdf}
%   }
%   \hfill
%   \subfloat[Compilation time in seconds\label{fig:spec-amd-compilation-time}]{
%     \includegraphics[width=0.45\linewidth]{figures/spec/spec-amd-compiletime.pdf}
%   }
%   \caption*{Targeting AMD Zen 3\label{fig:spec-amd}}
%   \caption{SPEC CPU2017 benchmark performance and compilation time}
%   \label{fig:spec}
% \end{figure*}

\begin{figure}[tbp]
  \centering
  \subfloat[Speedups on Cascade Lake\label{fig:spec-intel-speed-ups}]{
    \includegraphics[width=0.48\linewidth]{figures/spec-intel.pdf}
  }
  \hfill
  \subfloat[Speedups on Zen 3\label{fig:spec-amd-speed-ups}]{
    \includegraphics[width=0.48\linewidth]{figures/spec-amd.pdf}
  }
  \caption{SPEC CPU2017 benchmark performance, on a logarithmic scale}
  \label{fig:spec}
\end{figure}

\paragraph{Optimizing SPEC CPU 2017 with \minotaur{}}

Figure~\ref{fig:spec} shows the effect of optimizing the benchmarks
from SPEC CPU2017 using \minotaur.
%
When optimizing for, and running on, the Intel processor, we observed
a mean speedup of 1.5\%.
%
When optimizing for, and running on, the AMD processor, we observed a
mean speedup of 1.2\%.
%
It is notoriously difficult to speed up the SPEC CPU benchmarks
because compiler engineers have already put considerable effort into
achieving good code generation for them.


\subsection{Impact on Upstream LLVM}

In several cases where an optimization discovered by \minotaur{} seemed to
be simple and broadly applicable, we have reported its absence as an
LLVM defect, using the project's issue tracker.
%
This section summarizes the results of this informal LLVM-improvement
project.



%
Five of these (including the one we presented in Section~1) have now
been implemented in LLVM\@.
%
Three of them are in the code review phase: a patch exists and is
being discussed by developers.
%
Finally, two of them are being discussed, but a candidate patch does
not yet exist.


We also reported five missing vector optimizations.
%
One of these has been fixed, one has a patch that is under review, and
three are still being discussed.


\subsection{Optimizations Discovered by \minotaur}
\label{sec:examples}

The purpose of this section is to examine \minotaur's strengths by
presenting some optimizations that it found while compiling benchmark
programs.
%
None of these optimizations can be performed by the version of LLVM
that \minotaur{} is based on,\footnote{\minotaur{} uses LLVM~18.1.0 for all
results in this paper.}  at its \texttt{-O3} optimization level.
%
We present optimizations in an SSA format that is close to LLVM IR,
but we have edited it slightly for compactness and legibility.

\iffalse
One might be inclined to ask, while reading this section, ``Why is
LLVM incapable of performing this transformation?''
%
Alas, there is no single answer.
%
In some cases, performing the transformation would require the
optimizer to have a semantic model of a processor-specific intrinsic
function, but mostly these models do not exist.
%
In other cases, such as Example~5 below, generic reasoning about the
code would be very difficult, and a specific pattern matcher might not
be robust enough to be worth implementing.
%
Finally, our observation is that vector support in LLVM is somewhat
newer and less mature than support for other IR features, and the
optimizers have simply not had enough time to accumulate the requisite
optimizations.
\fi


\paragraph*{Example 1}

This code is from perlbench in SPEC:

{\begin{quote}\begin{verbatim}
%0 = zext <16 x i8> %x to <16 x i16>
%1 = zext <16 x i8> %y to <16 x i16>
%2 = call @llvm.x86.avx2.pavg.w(%0, %1)
%3 = trunc <16 x i16> %2 to <16 x i8>
ret <16 x i8> %3
  =>
%0 = call @llvm.x86.sse2.pavg.b(%x, %y)
ret <16 x i8> %0
\end{verbatim}
\end{quote}}

The unoptimized code zero-extends each 8-bit element of the two input
vectors to 16~bits, calls the AVX2 variant of \texttt{pavg} to perform
element-wise averaging of the extended vectors, and then truncates
elements of the resulting vector back to eight bits.
%
The optimized code simply calls an SSE2 version of the \texttt{pavg}
instruction that operates on 8-bit elements, reducing the uOp cost
of the operation from four to one.


\paragraph*{Example 2}
%https://godbolt.org/z/vjjr7MGzb

This code is from libYUV:

{\begin{quote}\begin{verbatim}
%0 = call @llvm.x86.avx2.pmadd.wd(%x, <0,1,0,1, ...>)
%1 = call @llvm.x86.avx2.pmadd.wd(%x, <1,0,1,0, ...>)
%2 = sub nsw <8 x i32> %1, %0
ret <8 x i32> %2
  =>
%0 = call @llvm.x86.avx2.pmadd.wd(%x,<1,-1,1,-1, ...>)
ret <8 x i32> %0
\end{verbatim}
\end{quote}}

The \texttt{pmadd.wd} (multiply and add packed integers) instruction multiplies
signed 16-bit integers element-wise from two input vectors, and then
computes its output by adding adjacent pairs of elements from the
resulting vector.
%
Thus, the input to this instruction is two 16-way vectors containing
16-bit elements, and its output is a single 8-way vector of 32-bit
elements.


In this example, the second argument to each \texttt{pmadd.wd}
instruction in the unoptimized code is a vector of alternating zeroes
and ones, which has the effect of selecting odd-indexed elements into
\texttt{\%0} and even-indexed elements into \texttt{\%1}.
%
Then, after the \texttt{sub} instruction, which simply performs
element-wise subtraction of \texttt{\%0} and \texttt{\%1}, the overall
effect of this code is to compute the difference between adjacent
pairs of elements of \texttt{\%x}.
%
\minotaur{} is able to perform this same computation using a single
\texttt{pmadd.wd} instruction which negates odd-numbered elements of
\texttt{\%x} before performing the addition.
%
The optimized code requires $5$ uOps to execute whereas the original
code requires $8$.


\paragraph*{Example 3}

This code is from libYUV:

%https://godbolt.org/z/7ooobqofK
{\begin{quote}\begin{verbatim}
%0 = shufflevector <32 x i8> %x, poison, <3, 7, 11, 15, 19, 23, 27, 31>
%1 = lshr %0, <6, 6, 6, 6, 6, 6, 6, 6>
%2 = zext 8 x i8> %1 to <8 x i32>
ret <8 x i32> %2
  =>
%0 = bitcast <32 x i8> %x to <8 x i32>
%1 = call @llvm.x86.avx2.psrli.d(<8 x i32> %0, 30)
ret <8 x i32> %1
\end{verbatim}
\end{quote}}

The \texttt{shufflevector} instruction in the unoptimized code selects
every fourth byte-sized element from the input \texttt{\%x}.
%
The resulting 8-way vector is right-shifted element-wise by six bit
positions, and that result is zero-extended to an 8-way vector of
32-bit elements.
%
\minotaur's optimized version (which executes in 4 uOps instead of 11)
first reinterprets the input vector's data as 32-bit elements; this
bitcast is relevant to LLVM's type system, but it is a nop at the CPU
level.
%
Then, the \texttt{prsli} instruction shifts each 32-bit element to the
right by 30 bit positions.
%
This right-shift-by-30 achieves the same effect as the unoptimized
code, where the \texttt{shufflevector} can be seen as a
right-shift-by-24, followed by an explicit right-shift-by-6.

\paragraph*{Example 4}

This code, from compiling perlbench from SPEC CPU 2017, illustrates
\minotaur's ability to reason about control flow:

% control flow divergence
%https://godbolt.org/z/e8jTsTMMz
{\begin{quote}\begin{verbatim}
entry:
  br i1 %c, label %body, label %if.end
body:
  br label %if.end
if.end:
  %p1 = phi [ %a, %body ], [ %b, %entry ]
  %p2 = phi [ %b, %body ], [ %a, %entry ]
  %r = call @llvm.x86.avx2.pavg.b(%p1, %p2)
  ret <32 x i8> %r
    =>
  %r = call @llvm.x86.avx2.pavg.b(%a, %b)
  ret <32 x i8> %r
\end{verbatim}
\end{quote}}

The intent of the code is to compute the element-wise average of input
vectors \texttt{\%a} and \texttt{\%b}, with a Boolean value
\texttt{\%c} determining the order in which the input vectors are
presented to the \texttt{pavg} instruction.
%
However, the order of arguments to this instruction does not matter, and
\minotaur's version executes in 4 uOps while the original code requires
10.
%
Note that \minotaur{} was not explicitly taught that \texttt{pavg} is
commutative; the necessary information was inferred naturally from the
formal specification.


\paragraph*{Example 5}

This is an optimization discovered
by \minotaur{} when it was used to compile GMP:

% before 19 after 13

{\begin{quote}\begin{verbatim}
%0 = lshr i64 %x, 1
%1 = and i64 %0, 0x5555555555555555
%2 = sub i64 %x, %1
%3 = lshr i64 %2, 2
%4 = and i64 %2, 0x3333333333333333
%5 = and i64 %3, 0x3333333333333333
%6 = add nuw nsw i64 %4, %3
%7 = lshr i64 %6, 4
%8 = add nuw nsw i64 %7, %6
%9 = and i64 %8, 0xf0f0f0f0f0f0f0f
ret i64 %9
  =>
%0 = bitcast i64 %x to <8 x i8>
%1 = call @llvm.ctpop(<8 x i8> %0)
%2 = bitcast <8 x i8> %1 to i64
ret i64 %2
\end{verbatim}
\end{quote}}

%
% \vspace{0.1in}
% %
% \caption{On the left, LLVM IR extracted from GMP; when compiled to
%   x86-64 code and run on an Intel Cascade Lake processor, its
%   predicted execution cost is 19 uOps. On the right, \tool's
%   optimized version of this code, which requires 13 uOps on the same
%   target.}
% \label{fig:ctpop}
% \end{figure*}
%
The original code performs a series of bit-level
manipulations on a 64-bit integer value, with the net result of
performing an 8-way vectorized 8-bit popcount operation.\footnote{The
popcount, or Hamming weight, of a bitvector is the number of ``1''
bits in it.}
%
The optimized code simply calls an intrinsic function to do the
popcount; it costs 13 uOps instead of the original code's 19.
%
Although robust recognition of open-coded idioms is not the focus
of our work, \minotaur{} does sometimes manage to achieve this.

Taking a strict view of types in the synthesis process could help
prune the search space, but it would also cause us to miss
optimizations that require a flexible view of types.
%
This example illustrates the latter case: the original code contains
no indication that a good optimization can be found using a vector of
type <8 x i8>, and therefore a strictly type-guided synthesis
procedure would miss this one.

\paragraph*{Example 6}

This code comes from 644.nab in SPEC CPU 2017:

% https://github.com/llvm/llvm-project/issues/85250
{\begin{quote}\begin{verbatim}
%0 = fcmp oge float %x, 0.000000e+00
%1 = fneg float %x
%2 = select i1 %0, float %0, float %2
%3 = fcmp oeq float %2, 0.000000e+00
ret i1 %3
  =>
%1 = fcmp oeq float %x, 0.000000e+00
ret i1 %oeq
\end{verbatim}
\end{quote}}

The original code computes the absolute value of a floating-point
number \texttt{\%x} and then checks if the result is zero.
\minotaur{} found that that the original code is equivalent to simply checking if
\texttt{\%x} is zero.


\paragraph*{Example 7}

This code comes from 619.lbm in SPEC CPU 2017:

% https://github.com/llvm/llvm-project/issues/85267

{\begin{quote}\begin{verbatim}
%0 = fmul float %x, 0x3FF0CCCCC0000000
%1 = fcmp olt float %t1, 0x3FE20418A0000000
ret i1 %1
  =>
%0 = fcmp ole float %x, 0x3FE12878E0000000
ret i1 %0
\end{verbatim}
\end{quote}}

The original code multiplies a floating-point value \texttt{\%x} by a
constant, and then checks if the result is less than another constant.
\minotaur{} found that this code is equivalent to checking if \texttt{\%x}
is less than or equal to a third constant.
%
This example shows that \minotaur{} can reason about and synthesize floating
point literals.


\paragraph*{Example 8}

This code comes from 638.imagick in SPEC CPU 2017:

{\begin{quote}\begin{verbatim}
%0 = fmul float %x, 0.000000e+00
%1 = fmul float %0, 3.000000e+00
ret float %1
  =>
%0 = fmul float %x, 0.000000e+00
ret i1 %0
\end{verbatim}
\end{quote}}

The original code multiplies a floating-point value \texttt{\%x} by
zero, and then multiplies the result by 3.0. \minotaur{} found that this
code is equivalent to multiplying \texttt{\%x} by zero directly.
Note the original code cannot be optimized to 0.0 directly, because of
the NaN and signed zero propagation rules in floating-point arithmetic.
This example shows that \minotaur{} is able to reason about these corner
cases and synthesize the correct code.

\paragraph*{Example 9}

This code comes from FlexC's benchmark suite:

{\begin{quote}\begin{verbatim}
%0 = extractelement <4 x ptr> %x, i32 0
%1 = extractelement <4 x ptr> %x, i32 3
%2 = load i32, ptr %0
%3 = load i32, ptr %1
%4 = insertelement <4 x i32> zeroinitializer, i32 %2, i32 0
%5 = insertelement <4 x i32> %4, i32 %3, i32 3
ret <4 x i32> %5
  =>
%0 = call @llvm.masked.gather(%0, 4, <true, false, false, true>, zeroinitializer)
ret <4 x i32> %0
\end{verbatim}
\end{quote}}

The original code extracts two pointers from a vector of pointers,
loads the values from these pointers, and then inserts these values
into a vector of integers. \minotaur{} found that this code is equivalent
to performing a masked gather operation, which loads values from
memory using a vector of pointers and a mask.