\section{Overview}
This section provides an overview of synthesizing latency- and
bandwidth-optimal algorithms, using \allgather for the \dgxone
topology~(Figure~\ref{fig:dgx1-topo}) as the running example.

\subsection{Collective Communication Primitives}
\label{sec:background-collectives}
Collective communication primitives allow nodes in a networked system
to perform operations on shared data. As an example, if each node has
some input data, the \allgather primitive transfers these data to all
of the nodes.  One way to implement this is for each node to
independently send its data to all other nodes. But, an algorithm in
which the nodes collectively work together can be more efficient. The
efficiency of such algorithms depends on the network topology.


\subsection{Topology}
The network topology specifies how the nodes are connected with each
other and the latency and bandwidth constraints on the links
connecting them. Consider the \dgxone topology shown in
Figure~\ref{fig:dgx1-topo}. It consists of $8$ GPUs (or nodes, in the
above formalism) split into two groups $\{0,1,2,3\}$ and
$\{4,5,6,7\}$. The nodes in each group are fully connected. In
addition, there are four inter-group links as shown in the figure.
These nodes are connected through NVLinks, with some nodes connected
with two parallel NVLinks as shown in Figure~\ref{fig:dgx1-topo}.

%There are two kinds of links shown in two different colors. The fast
%links, such as the one connecting nodes $0$ and $1$ have twice the
%bandwidth as the (relatively) slow links, such as the one connecting
%nodes $0$ and $2$. The figure represents the fast links with two
%arrows. Each link is bidirectional allowing concurrent sends and
%receives at full bandwidth. \todo{Remove the colors and the double
%arrow should make it clear} \todo{Also just mention that they are
%called nv2 and nv1 links} \todo{"split into two" groups, each
%associated with one CPU socket.} \todo{"All the nodes within a
%socket": all the GPUs in the same group} \todo{"inter-socket links"
%sound like QPI: maybe inter-group?} \todo{also check the usage in
%latency-optimal algorithm} \todo{I recommend talking about these in
%terms of groups rather than sockets to avoid the NVLink/PCIe/QPI
%confusion too -jn} \todo{I also recommend not talking about ``fast''
%and ``slow'' links, since the diagram shows a fast link as 2 slow
%links (which is indeed what it is), and we then talk about using 6
%logical rings; better to just say 6 links and 6 rings}

The \dgxone's design was heavily influenced by the need to do gradient
reduction for machine learning workloads. Specifically, this topology
forms two non-overlapping rings: one connecting nodes
$\{0,1,4,5,6,7,2,3\}$ with two NVLinks per edge and another connecting
$\{0,2,1,3,6,4,7,5\}$ with one NVLink per edge. These rings are
bidirectional and thus form $6$ logical single-NVLink rings. The NCCL
library implements \allgather by running $6$ simultaneous ring
algorithms as we discuss below.

%Given the fast ring has twice the bandwidth and each ring is
%bidirectional, this allows the implementation to utilize $6$ logical
%rings with the same bandwidth %to implement collection primitives (2
%from the slow ring and 4 from the fast ring).

\subsection{Cost Model}
%\todo{I think we need to define bandwidth and latency optimality
%here. My understanding is that a bandwidth-optimal algorithm is one
%that sends the minimal amount of data necessary to complete its
%operation, and a latency-optimal one uses the fewest number of
%communication rounds possible to complete its operation. Is that what
%we mean?}

%Before discussing how to build bandwidth- and latency-optimal
%algorithms for this topology, we will introduce a simple cost model.
We will characterize the communication cost using the $(\alpha,
\beta)$ model~\cite{hockney1994communication}. That is, sending a
message of size $L$ along a link costs $\alpha + L\cdot\beta$ time.
Here, $\alpha$ is the latency of communication and captures the {\em
fixed} costs, such as the overhead of initiating a transfer or
invoking a GPU kernel, and $\beta$ is the inverse bandwidth of the
link and captures {\em per-byte} costs, such as copying data into
system buffers. Li \etal{} extensively studies the transfer time of
buffers with different sizes over numerous GPU
interconnections\cite{alphabeta}. Their result show that with NVLinks,
the transfer time stays almost constant up-to a large buffer size and
only then it start to increase linearly. These results confirm that
the $(\alpha,\beta)$ model is suitable for characterizing
communication cost over NVLinks.

The cost of a collective algorithm for an input of size $L$ will be of
the form $a\cdot\alpha + b \cdot L \cdot \beta$. We call $a$ the {\em
latency cost} of the algorithm and $b$ the {\em bandwidth cost} of the
algorithm. Given a class of algorithms that implement a collective on
a given topology, an algorithm is {\em latency-optimal} ({\em
bandwidth-optimal}) if no other algorithm in the class has a lower
latency (bandwidth) cost. Usually, there is a tradeoff between the
latency cost and the bandwidth cost when designing collective
algorithms.  An algorithm with latency cost $a$ and bandwidth cost $b$
is said to be {\em Pareto-optimal} with respect to a class of
algorithms if for every algorithm in the class with latency cost $a'$
and bandwidth cost $b'$, we have $a = a' \Rightarrow b' \geq b$ and $b
= b' \Rightarrow  a' \geq a$.

%The $\alpha$ term represents the {\em fixed} cost of sending a
%message from the overhead of invoking a GPU kernel to the latency
%across the network. The $beta$ term represents from the cost of
%copying data across system buffers to the time taken to transmit the
%bytes at a given network bandwidth. For small message sizes, the cost
%is determined by the fixed cost $\alpha$, while for large message
%sizes, the cost is determined by the per-byte cost $\beta$. \todo{we
%are going to have a single kernel launch, so many not the kernel
%launch cost?} \todo{give our estimation of $\alpha$ and $\beta$ cost
%for DGX1 and NVLinks.} \todo{the from $\ldots$ to $\ldots$ in a long
%sentence is confusing: consider "such as"} \todo{note: I replaced
%``packet'' with ``message'', since alpha is more about the cost to
%initiate a transfer rather than to send a single 256-byte NVLink
%packet}

\subsection{Bandwidth-Optimal Algorithm for \dgxone}
\label{sec:motivation:bw-optimal}
As described above, the \dgxone topology has $6$ logical rings.
\allgather for one ring can be implemented as follows. Each node
simultaneously sends its data to the next node in the ring. In
subsequent steps, each node stores the received data and sends it to
the next node in the ring. In $7$ steps all nodes will have received
data from all of the other $7$ GPUs. The $6$-ring algorithm is a
generalization of this algorithm. Each node splits its data into $6$
chunks and executes the ring algorithm along each of the $6$ rings,
with one chunk per ring. If $L$ is the size of the input data, each
ring algorithm takes $7$ steps and communicates $\frac{L}{6}$ bytes.
Thus, the cost of the $6$-ring algorithm is
$$7\cdot \alpha + \frac{7}{6}\cdot L \cdot \beta.$$

Each node has to receive at least $7 \cdot L$ amount of data, and it
has an agglomerated incoming per-byte cost of $\beta/6$ (6 incoming
NVLinks). Thus, any algorithm for \allgather has to take at least
$\frac{7}{6}\cdot L \cdot \beta$ amount of time. Thus, this algorithm
is bandwidth-optimal for the \dgxone topology. But can we do better
with the latency cost?

Using the techniques described in this chapter, we have automatically
synthesized an algorithm~(Section~\ref{fig:dgxone:syn}) with cost
$$3\cdot \alpha + \frac{7}{6}\cdot L \cdot \beta.$$
%
To the best of our knowledge, this algorithm was not previously known.
Moreover, we prove that this algorithm is Pareto-optimal with respect
to the class of algorithms we call $k$-synchronous
algorithms~(Section~\ref{sec:ksync}).

\subsection{Latency-Optimal Algorithm for \dgxone}
The next question is whether we can improve upon the latency cost of
the synthesized algorithm. If each node communicates its data along a
binary tree instead of a ring, it would take at least $3$ steps. Using
the techniques described in this paper, we have automatically
synthesized a better algorithm~(Section~\ref{fig:dgxone:syn}) with
cost $$2\cdot \alpha + \frac{3}{2}\cdot L \cdot \beta.$$
%
Since the \dgxone topology has a diameter of $2$, this algorithm is
latency-optimal. To the best of our knowledge, a latency-optimal
algorithm for the \dgxone was not previously known. This algorithm is
Pareto-optimal with respect to the class of $k$-synchronous
algorithms.


%\subsection{Synthesizing Optimal Algorithms} \todofor{Olli}{Summarize
%our approach to synthesizing algorithms on the pareto frontier.}
%Given a topology, we automatically optimal algorithms in the pareto
%frontier.  Essentially a summary of the paper.
