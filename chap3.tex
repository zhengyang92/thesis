\chapter{Extending Alive2 for SIMD Verification and Synthesis}
\label{chap:extending-alive2}

In order for \minotaur{} to use Alive2 as a verification backend, I extended
Alive2 to support a number of floating point instructions,
platform independent vector instructions, and 165 x86 vector intrinsics.
%
I also extend Alive2 to support the verification of transformations on
bounded loops by unrolling loops in the source and target functions by
a specified factor.

\section{Supporting Floating Point Instructions}

LLVM's floating point instructions trivially map to SMT's Floating
Point Arithmetic (FPA) theory.
%
For example, a floating point addition \texttt{fadd} in LLVM IR is
translated to an \texttt{Z3_mk_fpa_add} API call in the Z3 solver.
%
Alive2 supports 16-bit (\texttt{half}), 32-bit (\texttt{float}), 64-bit
(\texttt{double}) and 128-bit (\texttt{fp128}) floating point types.
%
Non-IEEE-754 compliant floating point types are not supported, such as
the x86 80-bit floating type, as they are not supported by z3's FPA
theory.
%
Also not supported is \frem instruction, which is not supported by
Z3's FPA theory, either.

Alive2 supports \texttt{NNaN} (No NaNs), \texttt{NInf} (No Infinities) and
\texttt{NSZ} (No Signed Zeros) fast-math flags.

\begin{itemize}

\item \texttt{NNaN} flag allows optimizations to assume the arguments
and result are not \texttt{NaN} (not a number). If an argument is
\texttt{NaN}, or the result would be a NaN, the instruction produces a
poison value instead.

\item \texttt{NInf} flag allows optimizations to assume the arguments
and result are not positive or negative infinities. If an argument is
positive or negative infinities, or the result would be positive or
negative infinities, the instruction produces a poison value instead.

\item NSZ flag allows optimizations to treat the sign of a zero
argument or zero result as non-significant. Different from
\texttt{NNaN} and \texttt{NInf}, the \texttt{NSZ} flag does not imply
that the -0.0 is poison and/or guaranteed to not exist in the
operation.

\end{itemize}

Alive2 does not support the other fast-math flags, such as \texttt{ARCP},
\texttt{}

\section{Supporting Platform Independent SIMD operations}

LLVM uses a typed, SSA-based intermediate representation (IR).
%
It supports a derived \emph{vector type}; for example, a vector with
eight lanes, where each element is a 64-bit integer, would have type
\texttt{<8 x i64>}.
%
Many LLVM instructions, such as arithmetic operations, logical operations,
and pointer arithmetic, can operate on vectors as well as scalars.
%
IR-level vectors are target-independent; backends attempt to lower
vector operations to native SIMD instructions, if available.


Beyond the vertical ALU instructions that are element-wise vector
versions of scalar instructions, LLVM supports a variety of horizontal
vector reduction intrinsics and an assortment of memory intrinsics
such as vector load and store, strided load and store, and
scatter/gather.
%
Additionally, there are three vector-specific data movement
instructions:
%
\textit{extractelement} retrieves the element at a specified index from
a vector;
%
\textit{insertelement} non-destructively creates a new vector where
one element of an old vector has been replaced with a specified value;
%
and, \textit{shufflevector} returns a new vector that is a permutation
of two input vectors using elements whose indices are specified by a
constant mask vector.



\section{Supporting Intel Architecture Specific SIMD Intrinsics}

%
Finally, to provide direct access to platform-specific vector
instructions, LLVM provides numerous intrinsic functions such as
\texttt{@llvm.x86.avx512.mask.cvttps2dq.512}, aka ``convert with
truncation packed single-precision floating-point values to packed
signed doubleword integer values.''





The version of Alive2 that we started with supports most of the core
LLVM intermediate representation, including its target-independent
vector operations.
%
However, Alive2 did not have a semantics for any of the numerous
LLVM-level intrinsic functions that provide predictable, low-level
access to target-specific vector instructions.


We added semantics for 165 x86-64 vector intrinsics to Alive2; these
come from the SSE, AVX, AVX2, and AVX512 ISA extensions.
%
The resulting version of Alive2 supports the x86 vector intrinsics
that are widely used and that an SMT solver can reason about fairly
efficiently.
%
This includes special vertical operations that do not overlap with
LLVM's platform-independent vector instructions (such as
\texttt{psign}), special data movement intrinsics that operate
differently than LLVM's \texttt{shufflevector} (such as
\texttt{packsswb}), and special horizontal operations that are only
available in x86 processors (such as \texttt{vpdpbusd}). We have
excluded the intrinsics that are not commonly seen in practice, like
those altering control registers, that those such as dedicated
cryptographic operations, that an SMT solver is unlikely to be able to
make use of within a reasonable amount of CPU time.


There is significant overlapping functionality between vector
instructions; for example, there are eight different variants of the
\texttt{pavg} instruction that computes a vertical (element-wise)
average of two input vectors.
%
To exploit this overlap, our implementation is parameterized by vector
width, vector element size, and by the presence of a masking feature
that, when present, uses a bitvector to suppress the output of vector
results in some lanes.
%
Algorithms~\ref{semantic:pavg} and~\ref{semantic:pmadd} show our
implementations of the \texttt{pavg} (packed average) and
\texttt{pmadd.wd} (packed multiply and add) families of instructions.
%
This parameterized implementation enabled a high level of code reuse,
and our implementation of these semantics is only 660 lines of C++.
%
Note in particular that the semantics here differ from the semantics of
the corresponding processor instruction because at the LLVM level, we
must account for poison values---a form of deferred undefined
behavior.
%
Our strategy for dealing with poison follows the one used by existing
LLVM vector instructions: poison propagates lane-wise, but does not
contaminate non-dependent vector elements.

\renewcommand\algorithmicdo{}
\begin{algorithm}[tbp]
    \caption{semantics of \texttt{@llvm.x86.\{avx|avx2|avx512\}.pavg.\{b|w\}}}
    \label{semantic:pavg}
    \begin{algorithmic}[1]
    \State{\textsc{pavg$<$lanes, bits, masked$>g$}($S_{a}$, $S_{b}$, PassThrough, Mask)}
    \For{each $i$ in range [0 to \textsc{lanes} - 1]}
    \If {\textsc{masked} $\wedge \neg$ Mask[$i$] }
        \State {$S_{ret}$[$i$].val $\gets$ PassThrough[$i$].val}
        \State {$S_{ret}$[$i$].poison $\gets$ PassThrough[$i$].poison}
    \Else
        \State {$S_{ret}$[$i$].val $\gets$ ($S_{a}$[$i$].val $+_\textsc{\tiny{bits}}$ $S_{b}$[$i$].val $+_\textsc{\tiny{bits}}$ 1) /$_\textsc{\tiny{bits}}$ 2}
        \State {$S_{ret}$[$i$].poison $\gets S_{a}$[$i$].poison $\vee$  $S_{b}$[$i$].poison}
    \EndIf
    \EndFor
%    \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[tbp]
    \caption{semantics of \texttt{@llvm.x86.\{sse2|avx2|avx512\}.pmadd.wd}}
    \label{semantic:pmadd}
    \begin{algorithmic}[1]
    \State{\textsc{pmadd.wd$<$lanes, masked$>$}($S_{a}$, $S_{b}$, PassThrough, Mask)}
    \For{each $i$ in range [0 to \textsc{lanes}  - 1]}
    \If {\textsc{masked} $\wedge \neg$ Mask[$i$] }
        \State {$S_{ret}$[$i$].val $\gets$ PassThrough[$i$].val}
        \State {$S_{ret}$[$i$].poison $\gets$ PassThrough[$i$].poison}
    \Else
	\State {$S_{ret}$[$i$].val$\gets$sext($S_{a}$[2$\cdot i$].val $\times_\textsc{\tiny{16}}$ $S_{b}$[2$\cdot i$].val) $+_\textsc{\tiny{32}}$  sext($S_{a}$[2$\cdot i+1$].val $\times_\textsc{\tiny{16}}$ $S_{b}$[2$\cdot i +1$].val)}
	\State {$S_{ret}$[$i$].poison$\gets$$S_{a}$[2$\cdot i$].poison $\vee$  $S_{b}$[2$\cdot i$].poison $\vee$ $S_{a}$[2$\cdot i + 1$].poison $\vee$  $S_{b}$[2$\cdot i + 1$].poison}
    \EndIf
    \EndFor
    % \EndProcedure
    \end{algorithmic}
\end{algorithm}


\subsection{Validating our Changes to Alive2}

We made heavy use of randomized differential testing to ensure that
our new intrinsics correctly implement the intended semantics.
%
Each iteration of our tester randomly chooses constant inputs to a
single vector intrinsic and then:
%
\begin{enumerate}
\item
  Creates a small LLVM function passing the chosen inputs to the
  intrinsic.
\item
  Evaluates the effect of the function using LLVM's JIT compilation
  infrastructure~\cite{orc}. The effect is always to produce a
  concrete value, since the inputs are concrete.
\item
  Converts the LLVM function into Alive2 IR and then asks Alive2
  whether this is refined by the output of the JITted code.
\end{enumerate}
%
Any failure of refinement indicates a bug.
%
When we fielded this tester, it rapidly found $11$ cases
where our semantics produced an incorrect result, usually for
some edge case.
%
For example, the semantics for \texttt{pavg} were incorrect when the
sum overflowed.
%
It also found three cases where \minotaur{} generated SMT queries that
failed to typecheck.
%
For example, we set the wrong lane size when parameterizing the
semantics for \texttt{psra.w} and \texttt{psra.d}, causing the solver
to reject our malformed queries.
%
After we fixed these $14$ bugs, extensive testing failed to find
additional defects.



\section {Validating Loop Transformations by Unrolling}

Alive2 performs bounded translation validation by unrolling loops in
the source and target functions by a specified factor. Thus, it will
find any failure of refinement that is triggered within the specified
bound, and will miss refinement failures that require more loop
iterations to manifest. Alive2 implements Tarjan-Havlak’s loop
analysis algorithm [14] for recognizing the loops in a function and
their nesting relation. The result of the analysis is a loop nesting
forest (a collection of trees). In a nesting tree, each node
represents a loop header and its children are the headers of the
immediately nested loops.

Alive2 unrolls loops inside-out by traversing each loop nesting tree
in post-order with a DFS. An advantage of this order is that the
number of unrolls is linear in the number of loops and unroll factor
instead of being exponential if done in the reverse order.

Alive2 unrolls a loop by repeatedly duplicating the loop until the
unroll factor is reached. After duplication, three things need fixing:
(1) instruction operands, (2) targets of jump instructions, and (3)
introduce/patch $\phi$ nodes.

Operands are patched during basic block duplication. Alive2 maintains
a map that records the duplicates of each of the original SSA values.
When an instruction is duplicated, its operands are replaced with the
latest duplicate in this map.

Jump targets are patched by replacing each target with its next
duplicate. If no such basic block (BB) exists, it means it is a
backedge in the last unroll. We redirect these jumps to a special sink
BB. The reachability domain of the sink BB is negated and added to the
precondition of its respective function. Since the amount of unrolled
computation of the source/target functions may not be synchronized
(e.g., for loop-manipulating optimizations), we need to restrict
refinement checking such that the control flow cannot reach any of the
sink BBs. The usage of sink BBs allows us to avoid false positives,
but it prevents us from supporting infinite loops since we can only
check refinement of paths that reach a return instruction.

Some instructions inside a loop may have their result used outside the
loop. We need to patch such users so they observe the right values
depending on how many loop iterations are executed. We implemented a
conservative solution with three cases: patch existing $\phi$ nodes
(just add more predecessors), introduce a new $\phi$ when the loop has
a single exit to a BB that dominates [7] the user’s BB, and otherwise
fallback to using memory. Complex cases may require introducing
several $\phi$ nodes [3]; we introduce a new stack variable to avoid
having to maintain the SSA form altogether.


Picking the right unroll factor involves a tradeoff between coverage
and run time. We note that the unroll factor should be at least two
for optimizations that do not manipulate loops so we can cover the
backedge entry in $\phi$ nodes. For loop-manipulating optimizations,
this may have to go as high as 64, depending on the optimization.
Vectorization may optimize, e.g., 32 iterations of the source loop
into a single (vectorized) iteration, hence we need to unroll the
source loop at least 64 times so it covers two iterations of the
target loop.
