@inproceedings{Weiser81,
author = {Weiser, Mark},
title = {Program slicing},
year = {1981},
isbn = {0897911466},
publisher = {IEEE Press},
abstract = {Program slicing is a method used by experienced computer programmers for abstracting from programs. Starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. The reduced program, called a “slice”, is an independent program guaranteed to faithfully represent the original program within the domain of the specified subset of behavior.Finding a slice is in general unsolvable. A dataflow algorithm is presented for approximating slices when the behavior subset is specified as the values of a set of variables at a statement. Experimental evidence is presented that these slices are used by programmers during debugging. Experience with two automatic slicing tools is summarized. New measures of program complexity are suggested based on the organization of a program's slices.},
booktitle = {Proceedings of the 5th International Conference on Software Engineering},
pages = {439-–449},
numpages = {11},
keywords = {Software tools, Program metrics, Program maintenance, Human factors, Debugging, Data flow analysis},
location = {San Diego, California, USA},
series = {ICSE '81}
}

@article{minotaur,
author = {Liu, Zhengyang and Mada, Stefan and Regehr, John},
title = {{Minotaur: a SIMD-oriented synthesizing superoptimizer}},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {OOPSLA2},
url = {https://doi.org/10.1145/3689766},
doi = {10.1145/3689766},
abstract = {A superoptimizing compiler—-one that performs a meaningful search of the program space as part of the optimization process—-can find optimization opportunities that are missed by even the best existing optimizing compilers. We created Minotaur: a superoptimizer for LLVM that uses program synthesis to improve its code generation, focusing on integer and floating-point SIMD code. On an Intel Cascade Lake processor, Minotaur achieves an average speedup of 7.3\% on the GNU Multiple Precision library (GMP)’s benchmark suite, with a maximum speedup of 13\%. On SPEC CPU 2017, our superoptimizer produces an average speedup of 1.5\%, with a maximum speedup of 4.5\% for 638.imagick. Every optimization produced by Minotaur has been formally verified, and several optimizations that it has discovered have been implemented in LLVM as a result of our work.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {326},
numpages = {25},
pages={1561--1585},
keywords = {SIMD, peephole optimization, program synthesis, superoptimization}
}

@inproceedings{denali02,
author = {Joshi, Rajeev and Nelson, Greg and Randall, Keith},
title = {{Denali: a goal-directed superoptimizer}},
year = {2002},
isbn = {1581134630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/512529.512566},
doi = {10.1145/512529.512566},
abstract = {This paper provides a preliminary report on a new research project that aims to construct a code generator that uses an automatic theorem prover to produce very high-quality (in fact, nearly mathematically optimal) machine code for modern architectures. The code generator is not intended for use in an ordinary compiler, but is intended to be used for inner loops and critical subroutines in those cases where peak performance is required, no available compiler generates adequately efficient code, and where current engineering practice is to use hand-coded machine language. The paper describes the design of the superoptimizer, and presents some encouraging preliminary results.},
booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language Design and Implementation},
pages = {304-–314},
numpages = {11},
keywords = {superoptimizer, optimizing compiler},
location = {Berlin, Germany},
series = {PLDI '02}
}

@inproceedings{Dutertre15,
  author = {Dutertre, Bruno},
  title = {{Solving exists/forall problems with yices}},
  year = {2015},
  booktitle = {Proceedings of the International Workshop on Satisfiability Modulo Theories},
}

@article{peep84,
author = {Davidson, Jack W. and Fraser, Christopher W.},
title = {{Automatic generation of peephole optimizations}},
year = {2004},
issue_date = {April 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/989393.989407},
doi = {10.1145/989393.989407},
abstract = {This paper describes a system that automatically generates peephole optimizations. A general peephole optimizer driven by a machine description produces optimizations at compile-compile time for a fast, pattern-directed, compile-time optimizer. They form part of a compiler that simplifies retargeting by substituting peephole optimization for case analysis.},
journal = {SIGPLAN Not.},
month = apr,
pages = {104-–111},
numpages = {8}
}

@inproceedings{alive2,
author = {Lopes, Nuno P. and Lee, Juneyoung and Hur, Chung-Kil and Liu, Zhengyang and Regehr, John},
title = {{Alive2: bounded translation validation for LLVM}},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454030},
doi = {10.1145/3453483.3454030},
abstract = {We designed, implemented, and deployed Alive2: a bounded translation validation tool for the LLVM compiler’s intermediate representation (IR). It limits resource consumption by, for example, unrolling loops up to some bound, which means there are circumstances in which it misses bugs. Alive2 is designed to avoid false alarms, is fully automatic through the use of an SMT solver, and requires no changes to LLVM. By running Alive2 over LLVM’s unit test suite, we discovered and reported 47 new bugs, 28 of which have been fixed already. Moreover, our work has led to eight patches to the LLVM Language Reference—the definitive description of the semantics of its IR—and we have participated in numerous discussions with the goal of clarifying ambiguities and fixing errors in these semantics. Alive2 is open source and we also made it available on the web, where it has active users from the LLVM community.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {65-–79},
numpages = {15},
keywords = {Translation Validation, IR Semantics, Compilers, Automatic Software Verification},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{taming,
author = {Lee, Juneyoung and Kim, Yoonseung and Song, Youngju and Hur, Chung-Kil and Das, Sanjoy and Majnemer, David and Regehr, John and Lopes, Nuno P.},
title = {{Taming undefined behavior in LLVM}},
year = {2017},
isbn = {9781450349888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3062341.3062343},
doi = {10.1145/3062341.3062343},
abstract = {A central concern for an optimizing compiler is the design of its intermediate representation (IR) for code. The IR should make it easy to perform transformations, and should also afford efficient and precise static analysis. In this paper we study an aspect of IR design that has received little attention: the role of undefined behavior. The IR for every optimizing compiler we have looked at, including GCC, LLVM, Intel's, and Microsoft's, supports one or more forms of undefined behavior (UB), not only to reflect the semantics of UB-heavy programming languages such as C and C++, but also to model inherently unsafe low-level operations such as memory stores and to avoid over-constraining IR semantics to the point that desirable transformations become illegal. The current semantics of LLVM's IR fails to justify some cases of loop unswitching, global value numbering, and other important "textbook" optimizations, causing long-standing bugs. We present solutions to the problems we have identified in LLVM's IR and show that most optimizations currently in LLVM remain sound, and that some desirable new transformations become permissible. Our solutions do not degrade compile time or performance of generated code.},
booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {633-–647},
numpages = {15},
keywords = {compilers, intermediate representations, undefined behavior},
location = {Barcelona, Spain},
series = {PLDI '17}
}


@article{souper,
  title={{Souper: A synthesizing superoptimizer}},
  author={Sasnauskas, Raimondas and Chen, Yang and Collingbourne, Peter and Ketema, Jeroen and Lup, Gratian and Taneja, Jubi and Regehr, John},
  journal={arXiv preprint arXiv:1711.04422},
  year={2017}
}

@inproceedings{ithemal,
  title={{Ithemal: Accurate, portable and fast basic block throughput estimation using deep neural networks}},
  author={Mendis, Charith and Renda, Alex and Amarasinghe, Saman and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={4505--4515},
  year={2019},
}

@inproceedings{llvm,
author = {Lattner, Chris and Adve, Vikram},
title = {{LLVM: A Compilation Framework for Lifelong Program Analysis \& Transformation}},
year = {2004},
isbn = {0769521029},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper describes LLVM (Low Level Virtual Machine),a compiler framework designed to support transparent, lifelongprogram analysis and transformation for arbitrary programs,by providing high-level information to compilertransformations at compile-time, link-time, run-time, and inidle time between runs.LLVM defines a common, low-levelcode representation in Static Single Assignment (SSA) form,with several novel features: a simple, language-independenttype-system that exposes the primitives commonly used toimplement high-level language features; an instruction fortyped address arithmetic; and a simple mechanism that canbe used to implement the exception handling features ofhigh-level languages (and setjmp/longjmp in C) uniformlyand efficiently.The LLVM compiler framework and coderepresentation together provide a combination of key capabilitiesthat are important for practical, lifelong analysis andtransformation of programs.To our knowledge, no existingcompilation approach provides all these capabilities.We describethe design of the LLVM representation and compilerframework, and evaluate the design in three ways: (a) thesize and effectiveness of the representation, including thetype information it provides; (b) compiler performance forseveral interprocedural problems; and (c) illustrative examplesof the benefits LLVM provides for several challengingcompiler problems.},
booktitle = {Proceedings of the International Symposium on Code Generation and Optimization: Feedback-Directed and Runtime Optimization},
pages = {75},
location = {Palo Alto, California},
series = {CGO '04}
}

@inproceedings{ml_syn,
author = {Cowan, Meghan and Moreau, Thierry and Chen, Tianqi and Bornholt, James and Ceze, Luis},
title = {{Automatic generation of high-performance quantized machine learning kernels}},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377912},
doi = {10.1145/3368826.3377912},
abstract = {Quantization optimizes machine learning inference for resource constrained environments by reducing the precision of its computation. In the extreme, even single-bit computations can produce acceptable results at dramatically lower cost. But this ultra-low-precision quantization is difficult to exploit because extracting optimal performance requires hand-tuning both high-level scheduling decisions and low-level implementations. As a result, practitioners settle for a few predefined quantized kernels, sacrificing optimality and restricting their ability to adapt to new hardware. This paper presents a new automated approach to implementing quantized inference for machine learning models. We integrate the choice of how to lay out quantized values into the scheduling phase of a machine learning compiler, allowing it to be optimized in concert with tiling and parallelization decisions. After scheduling, we use program synthesis to automatically generate efficient low-level operator implementations for the desired precision and data layout. We scale up synthesis using a novel reduction sketch that exploits the structure of matrix multiplication. On a ResNet18 model, our generated code outperforms an optimized floating-point baseline by up to 3.9\texttimes{}, and a state-of-the-art quantized implementation by up to 16.6\texttimes{}.},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {305-–316},
numpages = {12},
keywords = {machine learning, quantization, synthesis},
location = {San Diego, CA, USA},
series = {CGO '20}
}

@inproceedings{rosette,
author = {Torlak, Emina and Bodik, Rastislav},
title = {{Growing solver-aided languages with rosette}},
year = {2013},
isbn = {9781450324724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509578.2509586},
doi = {10.1145/2509578.2509586},
abstract = {SAT and SMT solvers have automated a spectrum of programming tasks, including program synthesis, code checking, bug localization, program repair, and programming with oracles. In principle, we obtain all these benefits by translating the program (once) to a constraint system understood by the solver. In practice, however, compiling a language to logical formulas is a tricky process, complicated by having to map the solution back to the program level and extend the language with new solver-aided constructs, such as symbolic holes used in synthesis.This paper introduces ROSETTE, a framework for designing solver-aided languages. ROSETTE is realized as a solver-aided language embedded in Racket, from which it inherits extensive support for meta-programming. Our framework frees designers from having to compile their languages to constraints: new languages, and their solver-aided constructs, are defined by shallow (library-based) or deep (interpreter-based) embedding in ROSETTE itself.We describe three case studies, by ourselves and others, of using ROSETTE to implement languages and synthesizers for web scraping, spatial programming, and superoptimization of bitvector programs.},
booktitle = {Proceedings of the 2013 ACM International Symposium on New Ideas, New Paradigms, and Reflections on Programming \& Software},
pages = {135-–152},
numpages = {18},
keywords = {solver-aided languages},
location = {Indianapolis, Indiana, USA},
series = {Onward! '13}
}

@inproceedings{sparse,
author = {Horro, Marcos and Pouchet, Louis-No\"{e}l and Rodr\'{\i}guez, Gabriel and Touri\~{n}o, Juan},
title = {{Custom high-performance vector code generation for data-specific sparse computations}},
year = {2023},
isbn = {9781450398688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3559009.3569668},
doi = {10.1145/3559009.3569668},
abstract = {Sparse computations, such as sparse matrix-dense vector multiplication, are notoriously hard to optimize due to their irregularity and memory-boundedness. Solutions to improve the performance of sparse computations have been proposed, ranging from hardware-based such as gather-scatter instructions, to software ones such as generalized and dedicated sparse formats, used together with specialized executor programs for different hardware targets. These sparse computations are often performed on read-only sparse structures: while the data themselves are variable, the sparsity structure itself does not change. Indeed, sparse formats such as CSR have a typically high cost to insert/remove nonzero elements in the representation. The typical use case is to not modify the sparsity during possibly repeated computations on the same sparse structure.In this work, we exploit the possibility to generate a specialized executor program dedicated to the particular sparsity structure of an input matrix. It creates opportunities to remove indirection arrays and synthesize regular, vectorizable code for such computations. But, at the same time, it introduces challenges in code size and instruction generation, as well as efficient SIMD vectorization. We present novel techniques and extensive experimental results to efficiently generate SIMD vector code for data-specific sparse computations, and study the limits in terms of applicability and performance of our techniques compared to state-of-practice high-performance libraries like Intel MKL.},
booktitle = {Proceedings of the International Conference on Parallel Architectures and Compilation Techniques},
pages = {160-–171},
numpages = {12},
keywords = {data-specific compilation, sparse data structure, vectorization},
location = {Chicago, Illinois},
series = {PACT '22}
}

@article{slp,
author = {Larsen, Samuel and Amarasinghe, Saman},
title = {{Exploiting superword level parallelism with multimedia instruction sets}},
year = {2000},
issue_date = {May 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/358438.349320},
doi = {10.1145/358438.349320},
abstract = {Increasing focus on multimedia applications has prompted the addition
of multimedia extensions to most existing general purpose microprocessors.  This added functionality comes primarily with the addition of short SIMD instructions.  Unfortunately, access to these instructions is limited to in-line assembly and library calls. Generally, it has been assumed that vector compilers provide the most promising means of exploiting multimedia instructions. Although vectorization technology is well understood, it is inherently complex and fragile. In addition, it is incapable of locating SIMD-style parallelism within a basic block.In this paper we introduce the concept of Superword Level Parallelism (SLP) ,a novel way of viewing parallelism in multimedia and scientific applications. We believe SLPP is  fundamentally different from the loop level parallelism exploited by traditional vector processing, and therefore demands a new method of extracting it.  We have developed a simple and robust compiler for detecting SLPP that targets basic blocks rather than loop nests.  As with techniques designed to extract ILP, ours is able to exploit parallelism both across loop iterations and within basic blocks. The result is an algorithm that provides excellent performance in several application domains. In our experiments, dynamic instruction counts were reduced by 46\%. Speedups ranged from 1.24 to 6.70.},
journal = {SIGPLAN Not.},
month = may,
pages = {145-–156},
numpages = {12}
}

@inproceedings{rake,
author = {Ahmad, Maaz Bin Safeer and Root, Alexander J. and Adams, Andrew and Kamil, Shoaib and Cheung, Alvin},
title = {{Vector instruction selection for digital signal processors using program synthesis}},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507714},
doi = {10.1145/3503222.3507714},
abstract = {Instruction selection, whereby input code represented in an intermediate representation is translated into executable instructions from the target platform, is often the most target-dependent component in optimizing compilers. Current approaches include pattern matching, which is brittle and tedious to design, or search-based methods, which are limited by scalability of the search algorithm. In this paper, we propose a new algorithm that first abstracts the target platform instructions into high-level uber-instructions, with each uber-instruction unifying multiple concrete instructions from the target platform. Program synthesis is used to lift input code sequences into semantically equivalent sequences of uber-instructions and then to lower from uber-instructions to machine code. Using 21 real-world benchmarks, we show that our synthesis-based instruction selection algorithm can generate instruction sequences for a hardware target, with the synthesized code performing up to 2.1x faster as compared to code generated by a professionally-developed optimizing compiler for the same platform.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1004-–1016},
numpages = {13},
keywords = {Instruction selection, compiler optimizations, program synthesis},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{vegen,
author = {Chen, Yishen and Mendis, Charith and Carbin, Michael and Amarasinghe, Saman},
title = {{VeGen: a vectorizer generator for SIMD and beyond}},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446692},
doi = {10.1145/3445814.3446692},
abstract = {Vector instructions are ubiquitous in modern processors. Traditional compiler auto-vectorization techniques have focused on targeting single instruction multiple data (SIMD) instructions. However, these auto-vectorization techniques are not sufficiently powerful to model non-SIMD vector instructions, which can accelerate applications in domains such as image processing, digital signal processing, and machine learning. To target non-SIMD instruction, compiler developers have resorted to complicated, ad hoc peephole optimizations, expending significant development time while still coming up short. As vector instruction sets continue to rapidly evolve, compilers cannot keep up with these new hardware capabilities.  In this paper, we introduce Lane Level Parallelism (LLP), which captures the model of parallelism implemented by both SIMD and non-SIMD vector instructions. We present VeGen, a vectorizer generator that automatically generates a vectorization pass to uncover target-architecture-specific LLP in programs while using only instruction semantics as input. VeGen decouples, yet coordinates automatically generated target-specific vectorization utilities with its target-independent vectorization algorithm. This design enables us to systematically target non-SIMD vector instructions that until now require ad hoc coordination between different compiler stages. We show that VeGen can use non-SIMD vector instructions effectively, for example, getting speedup 3\texttimes{} (compared to LLVM’s vectorizer) on x265’s idct4 kernel.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {902–-914},
numpages = {13},
keywords = {auto-vectorization, non-SIMD, optimization},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@article{stoke,
author = {Schkufza, Eric and Sharma, Rahul and Aiken, Alex},
title = {{Stochastic superoptimization}},
year = {2013},
issue_date = {April 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/2499368.2451150},
doi = {10.1145/2499368.2451150},
abstract = {We formulate the loop-free binary superoptimization task as a stochastic search problem. The competing constraints of transformation correctness and performance improvement are encoded as terms in a cost function, and a Markov Chain Monte Carlo sampler is used to rapidly explore the space of all possible programs to find one that is an optimization of a given target program. Although our method sacrifices completeness, the scope of programs we are able to consider, and the resulting quality of the programs that we produce, far exceed those of existing superoptimizers. Beginning from binaries compiled by llvm -O0 for 64-bit x86, our prototype implementation, STOKE, is able to produce programs which either match or outperform the code produced by gcc -O3, icc -O3, and in some cases, expert handwritten assembly.},
journal = {SIGPLAN Not.},
month = mar,
pages = {305-–316},
numpages = {12},
keywords = {x86-64, x86, superoptimization, stochastic search, smt, mcmc, markov chain monte carlo, binary, 64-bit}
}

@article{Bansal06,
author = {Bansal, Sorav and Aiken, Alex},
title = {{Automatic generation of peephole superoptimizers}},
year = {2006},
issue_date = {November 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {11},
issn = {0362-1340},
url = {https://doi.org/10.1145/1168918.1168906},
doi = {10.1145/1168918.1168906},
abstract = {Peephole optimizers are typically constructed using human-written pattern matching rules, an approach that requires expertise and time, as well as being less than systematic at exploiting all opportunities for optimization. We explore fully automatic construction of peephole optimizers using brute force superoptimization. While the optimizations discovered by our automatic system may be less general than human-written counterparts, our approach has the potential to automatically learn a database of thousands to millions of optimizations, in contrast to the hundreds found in current peephole optimizers. We show experimentally that our optimizer is able to exploit performance opportunities not found by existing compilers; in particular, we show speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler.},
journal = {SIGPLAN Not.},
month = oct,
pages = {394-–403},
numpages = {10},
keywords = {superoptimization, peephole optimization, code selection}
}

@inproceedings{diospyros,
author = {VanHattum, Alexa and Nigam, Rachit and Lee, Vincent T. and Bornholt, James and Sampson, Adrian},
title = {{Vectorization for digital signal processors via equality saturation}},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446707},
doi = {10.1145/3445814.3446707},
abstract = {Applications targeting digital signal processors (DSPs) benefit from fast implementations of small linear algebra kernels. While existing auto-vectorizing compilers are effective at extracting performance from large kernels, they struggle to invent the complex data movements necessary to optimize small kernels. To get the best performance, DSP engineers must hand-write and tune specialized small kernels for a wide spectrum of applications and architectures. We present Diospyros, a search-based compiler that automatically finds efficient vectorizations and data layouts for small linear algebra kernels. Diospyros combines symbolic evaluation and equality saturation to vectorize computations with irregular structure. We show that a collection of Diospyros-compiled kernels outperform implementations from existing DSP libraries by 3.1\texttimes{} on average, that Diospyros can generate kernels that are competitive with expert-tuned code, and that optimizing these small kernels offers end-to-end speedup for a DSP application.},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {874-–886},
numpages = {13},
keywords = {Vectorization, Program Synthesis, Equality Saturation, DSPs},
location = {Virtual, USA},
series = {ASPLOS '21}
}

@inproceedings{swizzleinventor,
author = {Phothilimthana, Phitchaya Mangpo and Elliott, Archibald Samuel and Wang, An and Jangda, Abhinav and Hagedorn, Bastian and Barthels, Henrik and Kaufman, Samuel J. and Grover, Vinod and Torlak, Emina and Bodik, Rastislav},
title = {{Swizzle Inventor: Data movement synthesis for GPU kernels}},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304059},
doi = {10.1145/3297858.3304059},
abstract = {Utilizing memory and register bandwidth in modern architectures may require swizzles --- non-trivial mappings of data and computations onto hardware resources --- such as shuffles. We develop Swizzle Inventor to help programmers implement swizzle programs, by writing program sketches that omit swizzles and delegating their creation to an automatic synthesizer. Our synthesis algorithm scales to real-world programs, allowing us to invent new GPU kernels for stencil computations, matrix transposition, and a finite field multiplication algorithm (used in cryptographic applications). The synthesized 2D convolution and finite field multiplication kernels are on average 1.5--3.2x and 1.1--1.7x faster, respectively, than expert-optimized CUDA kernels.},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {65-–78},
numpages = {14},
keywords = {swizzling, program synthesis, GPGPU},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}


@article{massalin,
author = {Massalin, Henry},
title = {{Superoptimizer: a look at the smallest program}},
year = {1987},
issue_date = {Oct. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/36205.36194},
doi = {10.1145/36205.36194},
abstract = {Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the functions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor's instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole optimizers to improve the quality of generated code, or by assembly language programmers to improve manually written code.},
journal = {SIGPLAN Not.},
month = oct,
pages = {122-–126},
numpages = {5}
}

@article{halide,
author = {Ragan-Kelley, Jonathan and Adams, Andrew and Sharlet, Dillon and Barnes, Connelly and Paris, Sylvain and Levoy, Marc and Amarasinghe, Saman and Durand, Fr\'{e}do},
title = {{Halide: decoupling algorithms from schedules for high-performance image processing}},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {1},
issn = {0001-0782},
url = {https://doi.org/10.1145/3150211},
doi = {10.1145/3150211},
abstract = {Writing high-performance code on modern machines requires not just locally optimizing inner loops, but globally reorganizing computations to exploit parallelism and locality---doing things such as tiling and blocking whole pipelines to fit in cache. This is especially true for image processing pipelines, where individual stages do much too little work to amortize the cost of loading and storing results to and from off-chip memory. As a result, the performance difference between a naive implementation of a pipeline and one globally optimized for parallelism and locality is often an order of magnitude. However, using existing programming tools, writing high-performance image processing code requires sacrificing simplicity, portability, and modularity. We argue that this is because traditional programming models conflate the computations defining the algorithm with decisions about intermediate storage and the order of computation, which we call the schedule.We propose a new programming language for image processing pipelines, called Halide, that separates the algorithm from its schedule. Programmers can change the schedule to express many possible organizations of a single algorithm. The Halide compiler then synthesizes a globally combined loop nest for an entire algorithm, given a schedule. Halide models a space of schedules which is expressive enough to describe organizations that match or outperform state-of-the-art hand-written implementations of many computational photography and computer vision algorithms. Its model is simple enough to do so often in only a few lines of code, and small changes generate efficient implementations for x86, ARM, Graphics Processors (GPUs), and specialized image processors, all from a single algorithm.Halide has been public and open source for over four years, during which it has been used by hundreds of programmers to deploy code to tens of thousands of servers and hundreds of millions of phones, processing billions of images every day.},
journal = {Commun. ACM},
month = dec,
pages = {106-–115},
numpages = {10}
}

@article{peephole-gen,
author = {Davidson, Jack W. and Fraser, Christopher W.},
title = {{Automatic generation of peephole optimizations}},
year = {2004},
issue_date = {April 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/989393.989407},
doi = {10.1145/989393.989407},
abstract = {This paper describes a system that automatically generates peephole optimizations. A general peephole optimizer driven by a machine description produces optimizations at compile-compile time for a fast, pattern-directed, compile-time optimizer. They form part of a compiler that simplifies retargeting by substituting peephole optimization for case analysis.},
journal = {SIGPLAN Not.},
month = apr,
pages = {104-–111},
numpages = {8}
}


@inproceedings{optgen,
  title={{OPTGEN: A generator for local optimizations}},
  author={Buchwald, Sebastian},
  booktitle={Proceedings of the International Conference on Compiler Construction},
  pages={171--189},
  year={2015},
}

@inproceedings{sccl,
author = {Cai, Zixian and Liu, Zhengyang and Maleki, Saeed and Musuvathi, Madanlal and Mytkowicz, Todd and Nelson, Jacob and Saarikivi, Olli},
title = {{Synthesizing optimal collective algorithms}},
year = {2021},
isbn = {9781450382946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437801.3441620},
doi = {10.1145/3437801.3441620},
abstract = {Collective communication algorithms are an important component of distributed computation. Indeed, in the case of deep-learning, collective communication is the Amdahl's bottleneck of data-parallel training.This paper introduces SCCL (for Synthesized Collective Communication Library), a systematic approach to synthesizing collective communication algorithms that are explicitly tailored to a particular hardware topology. SCCL synthesizes algorithms along the Pareto-frontier spanning from latency-optimal to bandwidth-optimal implementations of a collective. The paper demonstrates how to encode the synthesis problem as a quantifier-free SMT formula which can be discharged to a theorem prover. We show how our carefully built encoding enables SCCL to scale.We synthesize novel latency and bandwidth optimal algorithms not seen in the literature on two popular hardware topologies. We also show how SCCL efficiently lowers algorithms to implementations on two hardware architectures (NVIDIA and AMD) and demonstrate competitive performance with hand optimized collective communication libraries.},
booktitle = {Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {62-–75},
numpages = {14},
keywords = {GPU, collective communication, interconnection, network, synthesis},
location = {Virtual Event, Republic of Korea},
series = {PPoPP '21}
}

@inproceedings{sketch,
author = {Solar-Lezama, Armando},
title = {{The sketching approach to program synthesis}},
year = {2009},
isbn = {9783642106712},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-10672-9_3},
doi = {10.1007/978-3-642-10672-9_3},
abstract = {Sketching is a new form of localized software synthesis that aims to bridge the gap between a programmer's high-level insights about a problem and the computer's ability to manage low-level details. In sketching, the programmer uses partial programs to describe the desired implementation <em>strategy</em> , and leaves the low-level details of the implementation to an automated synthesis procedure. This paper describes the sketching approach to program synthesis, including the details of the <Emphasis Type="SmallCaps">Sketch</Emphasis> language and synthesizer. The paper will then describe some of the techniques that make synthesis from sketches possible, and will close with a brief discussion of open problems in programmer guided synthesis.},
booktitle = {Proceedings of the 7th Asian Symposium on Programming Languages and Systems},
pages = {4-–13},
numpages = {10},
location = {Seoul, Korea},
series = {APLAS '09}
}


@article{equalitysat,
author = {Tate, Ross and Stepp, Michael and Tatlock, Zachary and Lerner, Sorin},
title = {{Equality saturation: a new approach to optimization}},
year = {2009},
issue_date = {January 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/1594834.1480915},
doi = {10.1145/1594834.1480915},
abstract = {Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer.},
journal = {SIGPLAN Not.},
month = jan,
pages = {264-–276},
numpages = {13},
keywords = {intermediate representation, equality reasoning, compiler optimization}
}

@inproceedings{z3,
author = {De Moura, Leonardo and Bj\o{}rner, Nikolaj},
title = {{Z3: an efficient SMT solver}},
year = {2008},
isbn = {3540787992},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.},
booktitle = {Proceedings of the Theory and Practice of Software, 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems},
pages = {337-–340},
numpages = {4},
location = {Budapest, Hungary},
series = {TACAS'08/ETAPS'08}
}


@inproceedings {laukemann,
author = { Laukemann, Jan and Hammer, Julian and Hager, Georg and Wellein, Gerhard },
booktitle = { 2019 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS) },
title = {{ Automatic throughput and critical path analysis of x86 and ARM assembly kernels }},
year = {2019},
volume = {},
ISSN = {},
pages = {1--6},
abstract = { Useful models of loop kernel runtimes on out-of-order architectures require an analysis of the in-core performance behavior of instructions and their dependencies. While an instruction throughput prediction sets a lower bound to the kernel runtime, the critical path defines an upper bound. Such predictions are an essential part of analytic (i.e., white-box) performance models like the Roofline and Execution-Cache-Memory (ECM) models. They enable a better understanding of the performance-relevant interactions between hardware architecture and loop code. The Open Source Architecture Code Analyzer (OSACA) is a static analysis tool for predicting the execution time of sequential loops. It previously supported only x86 (Intel and AMD) architectures and simple, optimistic full-throughput execution. We have heavily extended OSACA to support ARM instructions and critical path prediction including the detection of loop-carried dependencies, which turns it into a versatile cross-architecture modeling tool. We show runtime predictions for code on Intel Cascade Lake, AMD Zen, and Marvell ThunderX2 micro-architectures based on machine models from available documentation and semi-automatic benchmarking. The predictions are compared with actual measurements. },
keywords = {Throughput;Analytical models;Computer architecture;Kernel;Predictive models;Tools;Load modeling},
doi = {10.1109/PMBS49563.2019.00006},
url = {https://doi.ieeecomputersociety.org/10.1109/PMBS49563.2019.00006},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Nov}


@article{conditionally,
author = {Sharma, Rahul and Schkufza, Eric and Churchill, Berkeley and Aiken, Alex},
title = {{Conditionally correct superoptimization}},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858965.2814278},
doi = {10.1145/2858965.2814278},
abstract = {The aggressive optimization of heavily used kernels is an important problem in high-performance computing. However, both general purpose compilers and highly specialized tools such as superoptimizers often do not have sufficient static knowledge of restrictions on program inputs that could be exploited to produce the very best code. For many applications, the best possible code is conditionally correct: the optimized kernel is equal to the code that it replaces only under certain preconditions on the kernel's inputs. The main technical challenge in producing conditionally correct optimizations is in obtaining non-trivial and useful conditions and proving conditional equivalence formally in the presence of loops. We combine abstract interpretation, decision procedures, and testing to yield a verification strategy that can address both of these problems. This approach yields a superoptimizer for x86 that in our experiments produces binaries that are often multiple times faster than those produced by production compilers.},
journal = {SIGPLAN Not.},
month = oct,
pages = {147–-162},
numpages = {16},
keywords = {x86, Verification, Superoptimization, SMT, Optimization, Markov Chain Monte Carlo, Compilers, Binary Analysis}
}


 @article{woodruff2023rewriting,
  title={{Rewriting history: Repurposing domain-specific CGRAs}},
  author={Woodruff, Jackson and Koehler, Thomas and Brauckmann, Alexander and Cummins, Chris and Ainsworth, Sam and O'Boyle, Michael FP},
  journal={arXiv preprint arXiv:2309.09112},
  year={2023}
}

@article{hockney1994communication,
author = {Hockney, Roger W.},
title = {{The communication challenge for MPP: Intel Paragon and Meiko CS-2}},
year = {1994},
issue_date = {March 1994},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {20},
number = {3},
issn = {0167-8191},
url = {https://doi.org/10.1016/S0167-8191(06)80021-9},
doi = {10.1016/S0167-8191(06)80021-9},
abstract = {The communication performance of the Intel iPSC/860, Paragon XP/S and the Meiko CS-2 are compared using the COMMS1 benchmark from the Genesis Parallel Benchmark Suite. The challenge to distributed-memory massively-parallel processors presented by the Cray-C90 shared memory computer is highlighted by re-interpreting vector processing results as though they were measuring communication startup and bandwidth. The results show a wide gap between the two types of computer, in favour of the C-90. These results are for the initial issue of software and hardware of the Paragon and CS-2. Comments from Intel and Meiko are included to show how the manufacturers aim to improve communication performance.},
journal = {Parallel Comput.},
month = mar,
pages = {389--398},
numpages = {10},
keywords = {Meiko CS-2, Massively-parallel processors, Latency, Intel iPSC/860, Intel Paragon, Cray-C90, Communication performance, COMMS1 benchmark}
}

@article{chan2007collective,
author = {Chan, Ernie and Heimlich, Marcel and Purkayastha, Avi and van de Geijn, Robert},
title = {{Collective communication: theory, practice, and experience: Research Articles}},
year = {2007},
issue_date = {September 2007},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {19},
number = {13},
issn = {1532-0626},
abstract = {We discuss the design and high-performance implementation of collective communications operations on distributed-memory computer architectures. Using a combination of known techniques (many of which were first proposed in the 1980s and early 1990s) along with careful exploitation of communication modes supported by MPI, we have developed implementations that have improved performance in most situations compared to those currently supported by public domain implementations of MPI such as MPICH. Performance results from a large Intel Xeon/Pentium 4 (R) processor cluster are included. Copyright © 2007 John Wiley \& Sons, Ltd.},
journal = {Concurr. Comput. :  Pract. Exper.},
month = sep,
pages = {1749–-1783},
numpages = {35},
keywords = {clusters, collective communication, distributed-memory architecture}
}

@inproceedings{pjevsivac2007performance,
  author={Pje{\v{s}}ivac-Grbovi{\'c}, Jelena and Angskun, Thara and Bosilca, George and Fagg, Graham E and Gabriel, Edgar and Dongarra, Jack J},
  booktitle={19th IEEE International Parallel and Distributed Processing Symposium},
  title={{Performance analysis of MPI collective operations}},
  year={2005},
  volume={},
  number={},
  pages={8},
  keywords={Performance analysis;High performance computing;Predictive models;Libraries;Topology;System testing;Lifting equipment;Laboratories;Computer science;Application software},
  doi={10.1109/IPDPS.2005.335}}

@article{mpich,
author = {Gropp, William and Lusk, Ewing and Doss, Nathan and Skjellum, Anthony},
title = {{A high-performance, portable implementation of the MPI message passing interface standard}},
year = {1996},
issue_date = {Sept. 1996},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {22},
number = {6},
issn = {0167-8191},
url = {https://doi.org/10.1016/0167-8191(96)00024-5},
doi = {10.1016/0167-8191(96)00024-5},
journal = {Parallel Comput.},
month = sep,
pages = {789--828},
numpages = {40},
keywords = {portability, performance, parallel programming environment, message passing interface, benchmark, MPI-2}
}

@article{thakur2005optimization,
  title={{Optimization of collective communication operations in MPICH}},
  author={Thakur, Rajeev and Rabenseifner, Rolf and Gropp, William},
  journal={The International Journal of High Performance Computing Applications},
  volume={19},
  number={1},
  pages={49--66},
  year={2005},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}


@article{barnett1993global,
author = {Barnett, M. and Littlefield, R. and Payne, D.G. and Vandegeijn, R.},
title = {{Global combine algorithms for 2-D meshes with wormhole routing}},
year = {1995},
issue_date = {Feb. 1, 1995},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {24},
number = {2},
issn = {0743-7315},
url = {https://doi.org/10.1006/jpdc.1995.1018},
doi = {10.1006/jpdc.1995.1018},
abstract = {The problem of performing a global combine (summation) operation on a distributed memory computer using a two-dimensional mesh interconnect with wormhole routing is considered. We present algorithms that are asymptotically optimal for short vectors (O(log(p)) for p processing nodes) and for long vectors (O(n) for n data elements per node), as well as hybrid algorithms that are superior for intermediate n. The algorithms are analyzed using detailed performance models that include the effects of link conflicts and other characteristics of the underlying communication system. The models are validated using experimental data from the Intel Touchstone DELTA computer. We show that no one algorithm is optimal for all vector lengths; rather, each of the presented algorithms is superior under some circumstances.},
journal = {J. Parallel Distrib. Comput.},
month = feb,
pages = {191-–201},
numpages = {11}
}


@INPROCEEDINGS{scott1991efficient,
  author={Scott, David S},
  booktitle={The Sixth Distributed Memory Computing Conference, 1991. Proceedings},
  title={{Efficient all-to-all communication patterns in hypercube and mesh topologies}},
  year={1991},
  volume={},
  number={},
  pages={398--403},
  keywords={Hypercubes;Topology;Routing;Concurrent computing;Distributed computing;Application software;Processor scheduling;Switching circuits;Optimal scheduling;Wire},
  doi={10.1109/DMCC.1991.633174}}


@INPROCEEDINGS{bokhari1992complete,
  author={Bokhari, Shahid H and Berryman, Harry},
  booktitle={Proceedings Scalable High Performance Computing Conference SHPCC-92.},
  title={{Complete exchange on a circuit switched mesh}},
  year={1992},
  volume={},
  number={},
  pages={300--306},
  keywords={Switching circuits;Hypercubes;Communication switching;Programming profession;Power engineering computing;NASA;Distributed computing;Power engineering and energy;Hardware;Integrated circuit interconnections},
  doi={10.1109/SHPCC.1992.232628}}


@inproceedings{sanders2002hierarchical,
author = {Sanders, Peter and Tr\"{a}ff, Jesper Larsson},
title = {{The hierarchical factor algorithm for all-to-all communication}},
year = {2002},
isbn = {3540440496},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present an algorithm for regular, personalized all-to-all communication, in which every processor has an individual message to deliver to every other processor. Our machine model is a cluster of processing nodes where each node, possibly consisting of several processors, can participate in only one communication operation with another node at a time. The nodes may have different numbers of processors. This general model is important for the implementation of all-to-all communication in libraries such as MPI where collective communication may take place over arbitrary subsets of processors. The algorithm is optimal up to an additive term that is small if the total number of processors is large compared to the maximal number of processors in a node.},
booktitle = {Proceedings of the 8th International Euro-Par Conference on Parallel Processing},
pages = {799-–804},
numpages = {6},
series = {Euro-Par '02}
}

@inproceedings{sistare1999optimization,
author = {Sistare, Steve and vandeVaart, Rolf and Loh, Eugene},
title = {{Optimization of MPI collectives on clusters of large-scale SMP's}},
year = {1999},
isbn = {1581130910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/331532.331555},
doi = {10.1145/331532.331555},
booktitle = {Proceedings of the 1999 ACM/IEEE Conference on Supercomputing},
pages = {23},
keywords = {shared memory, collective, clustering, SMP, MPICH, MPI},
location = {Portland, Oregon, USA},
series = {SC '99}
}


@inproceedings{tipparaju2003fast,
author = {Tipparaju, Vinod and Nieplocha, Jarek and Panda, Dhabaleswar},
title = {{Fast collective operations using shared and remote memory access protocols on clusters}},
year = {2003},
isbn = {0769519261},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper describes a novel methodology for implementing a common set of collective communication operations on clusters based on symmetric multiprocessor (SMP) nodes. Called Shared-Remote-Memory collectives, or SRM, our approach replaces the point-to-point message passing, traditionally used in implementation of collective message-passing operations, with a combination of shared and remote memory access (RMA) protocols that are used to implement semantics of the collective operations directly. Appropriate embedding of the communication graphs in a cluster maximizes the use of shared memory and reduces network communication. Substantial performance improvements are achieved over the highlyoptimized commercial IBM implementation and the open- source MPICH implementation of MPI across a wide range of message sizes on the IBM SP. For example, depending on the message size and number of processors, SRM implementation of broadcast, reduce, and barrier outperforms IBM MPI_Bcast by 27-84\%, MPI_Reduce by 24- 79\%, and MPI_Barrier by 73\% on 256 processors, respectively.},
booktitle = {Proceedings of the 17th International Symposium on Parallel and Distributed Processing},
pages = {84},
series = {IPDPS '03}
}

@inproceedings{traff2002improved,
author = {Tr\"{a}ff, Jesper Larsson},
title = {{Improved MPI all-to-all communication on a giganet SMP cluster}},
year = {2002},
isbn = {3540442960},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present the implementation of an improved, almost optimal algorithm for regular, personalized all-to-all communication for hierarchical multiprocessors, like clusters of SMP nodes. In MPI this communication primitive is realized in the MPI_Alltoall collective. The algorithm is a natural generalization of a well-known algorithm for nonhierarchical systems based on factorization. A specific contribution of the paper is a completely contention-free scheme not using token-passing for exchange of messages between SMP nodes.We describe a dedicated implementation for a small Giganet SMP cluster with 6 SMP nodes of 4 processors each. We present simple experiments to validate the assumptions underlying the design of the algorithm. The results were used to guide the detailed implementation of a crucial part of the algorithm. Finally, we compare the improved MPI_Alltoall collective to a trivial (but widely used) implementation, and show improvements in average completion time of sometimes more than 10\%. While this may not seem much, we have reasons to believe that the improvements will be more substantial for larger systems.},
booktitle = {Proceedings of the 9th European PVM/MPI Users' Group Meeting on Recent Advances in Parallel Virtual Machine and Message Passing Interface},
pages = {392-–400},
numpages = {9}
}


@inproceedings{bruck1997efficient,
author = {Bruck, Jehoshua and Ho, Ching-Tien and Kipnis, Shlomo and Weathersby, Derrick},
title = {{Efficient algorithms for all-to-all communications in multi-port message-passing systems}},
year = {1994},
isbn = {0897916719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/181014.181756},
doi = {10.1145/181014.181756},
abstract = {We present efficient algorithms for two all-to-all communication operations in message-passing systems: index (or all-to-all personalized communication) and concatenation (or all-to-all broadcast). We assume a model of a fully-connected message-passing system, in which the performance of any point-to-point communication is independent of the sender-receiver pair. We also assume that each processor has k ≥ 1 ports, through which it can send and receive k messages in every communication round. The complexity measures we use are independent of the particular system topology and are based on the communication start-up time and on the communication bandwidth.In the index operation among n  processors, initially, each processor has n blocks of data, and the goal is to exchange the i-th block of processor j with the j-th block of processor i. We present a class of index algorithms that is designed for all values of n and that features a trade-off between the communication of start-up time and the data transfer time. This class of algorithms includes two special cases: an algorithm that is optimal with respect to the measure of the start-up time, and an algorithm that is optimal with respect to the measure of the data transfer time. We also present experimental results featuring the performance tuneability of our index algorithms on the IBM SP-1 parallel system.In  the concatenation operation among n processors, initially, each processor has one block of data, and the goal is to concatenate the n blocks of data from the n processors and to make the concatenation result known to all the processors. We present a concatenation algorithm that is optimal, for most values of n, in the number of communication rounds and in the amount of data transferred.},
booktitle = {Proceedings of the Sixth Annual ACM Symposium on Parallel Algorithms and Architectures},
pages = {298--309},
numpages = {12},
location = {Cape May, New Jersey, USA},
series = {SPAA '94}
}


@inproceedings{gabriel2004open,
  title={{Open MPI: Goals, concept, and design of a next generation MPI implementation}},
  author={Gabriel, Edgar and Fagg, Graham E and Bosilca, George and Angskun, Thara and Dongarra, Jack J and Squyres, Jeffrey M and Sahay, Vishal and Kambadur, Prabhanjan and Barrett, Brian and Lumsdaine, Andrew and others},
  booktitle={Proceedings of the European Parallel Virtual Machine/Message Passing Interface Users’ Group Meeting},
  pages={97--104},
  year={2004},
}

@inproceedings{barnett1994building,
author = {Barnett, Mike and Gupta, Satya and Payne, David G. and Shuler, Lance and van de Geijn, Robert and Watts, Jerrell},
title = {{Building a high-performance collective communication library}},
year = {1994},
isbn = {0818666056},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {In this paper, we report on a project to develop a unified approach for building a library of collective communication operations that performs well on a cross-section of problems encountered in real applications. The target architecture is a two-dimensional mesh with worm-hole routing, but the techniques are more general. The approach differs from traditional library implementations in that we address the need for implementations that perform well for various sized vectors and grid dimensions, including non-power-of-two grids. We show how a general approach to hybrid algorithms yields performance across the entire range of vector lengths. Moreover, many scalable implementations of application libraries require collective communication within groups of nodes. Our approach yields the same kind of performance for group collective communication. Results from the Intel Paragon system are included. To obtain this library for Intel systems contact intercom©cs.utexas.edu.},
booktitle = {Proceedings of the 1994 ACM/IEEE Conference on Supercomputing},
pages = {107-–116},
numpages = {10},
location = {Washington, D.C.},
series = {Supercomputing '94}
}

@inproceedings{wangt2018blink,
  title={{Blink: A fast NVLink-based collective communication library}},
  author={Wangt, Guanhua and Phanishayee, Amar and Venkataraman, Shivaram and Stoicat, Ion},
  booktitle={{Proceedings of the Conference on Machine Learning and Systems}},
  year={2018}
}

@article{wang2020blink,
  title={{Blink: fast and generic collectives for distributed ML}},
  author={Wang, Guanhua and Venkataraman, Shivaram and Phanishayee, Amar and Thelin, Jorgen and Devanur, Nikhil and Stoica, Ion},
  journal={arXiv preprint arXiv:1910.04940},
  year={2019}
}

@inproceedings{zhang2017poseidon,
author = {Zhang, Hao and Zheng, Zeyu and Xu, Shizhen and Dai, Wei and Ho, Qirong and Liang, Xiaodan and Hu, Zhiting and Wei, Jinliang and Xie, Pengtao and Xing, Eric P.},
title = {{Poseidon: an efficient communication architecture for distributed deep learning on GPU clusters}},
year = {2017},
isbn = {9781931971386},
publisher = {USENIX Association},
address = {USA},
abstract = {Deep learning models can take weeks to train on a single GPU-equipped machine, necessitating scaling out DL training to a GPU-cluster. However, current distributed DL implementations can scale poorly due to substantial parameter synchronization over the network, because the high throughput of GPUs allows more data batches to be processed per unit time than CPUs, leading to more frequent network synchronization. We present Poseidon, an efficient communication architecture for distributed DL on GPUs. Poseidon exploits the layered model structures in DL programs to overlap communication and computation, reducing bursty network communication. Moreover, Poseidon uses a hybrid communication scheme that optimizes the number of bytes required to synchronize each layer, according to layer properties and the number of machines. We show that Poseidon is applicable to different DL frameworks by plugging Poseidon into Caffe and TensorFlow. We show that Poseidon enables Caffe and TensorFlow to achieve 15.5\texttimes{} speed-up on 16 single-GPU machines, even with limited bandwidth (10GbE) and the challenging VGG19-22K network for image classification. Moreover, Poseidon-enabled TensorFlow achieves 31.5\texttimes{} speed-up with 32 single-GPU machines on Inception-V3, a 50\% improvement over the open-source TensorFlow (20\texttimes{} speed-up).},
booktitle = {Proceedings of the 2017 USENIX Conference on Usenix Annual Technical Conference},
pages = {181-–193},
numpages = {13},
location = {Santa Clara, CA, USA},
series = {USENIX ATC '17}
}


@inproceedings{jayarajan2019priority,
  title={{Priority-based parameter propagation for distributed DNN training}},
  author={Jayarajan, Anand and Wei, Jinliang and Gibson, Garth and Fedorova, Alexandra and Pekhimenko, Gennady},
  booktitle = {Proceedings of the Conference on Machine Learning and Systems},
  year = {2019},
  pages={132--145},
}

@inproceedings{hashemi2019tictac,
  title={{TicTac: Accelerating distributed deep learning with communication scheduling}},
  author={Hashemi, Sayed Hadi and Jyothi, Sangeetha Abdu and Campbell, Roy H},
  booktitle = {Proceedings of the Conference on Machine Learning and Systems},
  year = {2019},
  pages={418--430},
}

@inproceedings{peng2019generic,
author = {Peng, Yanghua and Zhu, Yibo and Chen, Yangrui and Bao, Yixin and Yi, Bairen and Lan, Chang and Wu, Chuan and Guo, Chuanxiong},
title = {{A generic communication scheduler for distributed DNN training acceleration}},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359642},
doi = {10.1145/3341301.3359642},
abstract = {We present ByteScheduler, a generic communication scheduler for distributed DNN training acceleration. ByteScheduler is based on our principled analysis that partitioning and rearranging the tensor transmissions can result in optimal results in theory and good performance in real-world even with scheduling overhead. To make ByteScheduler work generally for various DNN training frameworks, we introduce a unified abstraction and a Dependency Proxy mechanism to enable communication scheduling without breaking the original dependencies in framework engines. We further introduce a Bayesian Optimization approach to auto-tune tensor partition size and other parameters for different training models under various networking conditions. ByteScheduler now supports TensorFlow, PyTorch, and MXNet without modifying their source code, and works well with both Parameter Server (PS) and all-reduce architectures for gradient synchronization, using either TCP or RDMA. Our experiments show that ByteScheduler accelerates training with all experimented system configurations and DNN models, by up to 196\% (or 2.96X of original speed).},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {16–-29},
numpages = {14},
keywords = {ML frameworks, communication scheduling},
location = {Huntsville, Ontario, Canada},
series = {SOSP '19}
}

@INPROCEEDINGS{gdrcopy,
  author={Shi, Rong and Potluri, Sreeram and Hamidouche, Khaled and Perkins, Jonathan and Li, Mingzhe and Rossetti, Davide and Panda, Dhabaleswar K. D K},
  booktitle={Proceedings of the International Conference on High Performance Computing},
  title={{Designing efficient small message transfer mechanism for inter-node MPI communication on InfiniBand GPU clusters}},
  year={2014},
  volume={},
  number={},
  pages={1--10},
  keywords={Graphics processing units;Protocols;Bandwidth;Libraries;Receivers;Benchmark testing;Performance evaluation;MPI;CUDA;InfiniBand;GPU Direct RDMA},
  doi={10.1109/HiPC.2014.7116873}}


@article{alex2018horovod,
  title={{Horovod: fast and easy distributed deep learning in TensorFlow}},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}

@ARTICLE{blueconnect,
  author={Cho, Minsik and Finkler, Ulrich and Serrano, Mauricio and Kung, David and Hunter, Hillery},
  journal={IBM Journal of Research and Development},
  title={{BlueConnect: decomposing all-reduce for deep learning on heterogeneous network hierarchy}},
  year={2019},
  volume={63},
  number={6},
  pages={1--11},
  keywords={Deep learning;Heterogeneous networks;Training;Bandwidth;Synchronization;Neural networks;Libraries},
  doi={10.1147/JRD.2019.2947013}}

@inproceedings{plink,
 title = {PLink: Discovering and exploiting locality for accelerated distributed training on the public cloud},
 author = {Luo, Liang and West, Peter and Nelson, Jacob and Krishnamurthy, Arvind and Ceze, Luis},
 booktitle = {Proceedings of the Conference on Machine Learning and Systems},
 pages = {82--97},
 year = {2020}
}

@article{havlak,
author = {Havlak, Paul},
title = {{Nesting of reducible and irreducible loops}},
year = {1997},
issue_date = {July 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {4},
issn = {0164-0925},
url = {https://doi.org/10.1145/262004.262005},
doi = {10.1145/262004.262005},
abstract = {Recognizing and transforming loops are essential steps in any attempt to improve the running time of a program. Aggressive restructuring techniques have been developed for single-entry (reducible) loops, but restructurers and the dataflow and dependence analysis they rely on often give up in the presence of multientry (irreducible) loops. Thus one irreducible loop can prevent the improvement of all loops in a procedure. This article give an algorithm to build a loop nesting tree for a procedure with arbitrary control flow. The algorithm uses definitions of reducible and irreducible loops which allow either kind of loop to be nested in the other. The tree construction algorithm, an extension of Tarjan's algorithm  for testing reducibility, runs in almost linear time. In the presence        of irreducible loops, the loop nesting tree can depend on the depth-first spanning tree used to build it. In particular, the header node representing a reducible loop in one version of the loop nesting tree can be the representative of an irreducible loop in another. We give a normalization method that maximizes the set of reducible loops discovered, independent of the depth-first spanning tree used. The normalization require the insertion of at most one node and one edge per reducible loop.},
journal = {ACM Trans. Program. Lang. Syst.},
month = jul,
pages = {557–-567},
numpages = {11},
keywords = {strongly-connected regions, reducible loops}
}

@inproceedings{difftune,
  author={Renda, Alex and Chen, Yishen and Mendis, Charith and Carbin, Michael},
  booktitle={Proceedings of the IEEE/ACM International Symposium on Microarchitecture},
  title={{DiffTune: optimizing CPU simulator parameters with learned differentiable surrogates}},
  year={2020},
  volume={},
  number={},
  pages={442--455},
  keywords={Microarchitecture;Tools;Complexity theory;Software measurement;Task analysis;Optimization;Tuning;Performance Modeling;Machine Learning},
  doi={10.1109/MICRO50266.2020.00045}}



@article{alphabeta,
author = {Li, Ang and Song, Shuaiwen Leon and Chen, Jieyang and Li, Jiajia and Liu, Xu and Tallent, Nathan R. and Barker, Kevin J.},
title = {{Evaluating modern GPU interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect}},
year = {2020},
issue_date = {Jan. 2020},
publisher = {IEEE Press},
volume = {31},
number = {1},
issn = {1045-9219},
url = {https://doi.org/10.1109/TPDS.2019.2928289},
doi = {10.1109/TPDS.2019.2928289},
abstract = {High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink's topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efficiency, as well as the application's overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning.},
journal = {IEEE Trans. Parallel Distrib. Syst.},
month = jan,
pages = {94–-110},
numpages = {17}
}

@incollection{barrett2018satisfiability,
  title = {{Satisfiability modulo theories}},
  author = {Barrett, Clark and Tinelli, Cesare},
  booktitle = {Handbook of model checking},
  pages={305--343},
  year={2018},
  publisher={Springer}
}


@inproceedings{OpenMPI,
author = {Hursey, Joshua and Mallove, Ethan and Squyres, Jeffrey M. and Lumsdaine, Andrew},
title = {{An extensible framework for distributed testing of MPI implementations}},
year = {2007},
isbn = {3540754156},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Complex code bases require continual testing to ensure that both new development and routine maintenance do not create unintended side effects. Automation of regression testing is a common mechanism to ensure consistency, accuracy, and repeatability of results. The MPI Testing Tool (MTT) is a flexible framework specifically designed for testing MPI implementations across multiple organizations and environments. The MTT offers a unique combination of features not available in any individual testing framework, including a built-in multiplicative effect for creating and running tests, historical correctness and performance analysis, and support for multiple cluster resource managers.},
booktitle = {Proceedings of the 14th European Conference on Recent Advances in Parallel Virtual Machine and Message Passing Interface},
pages = {64-–72},
numpages = {9},
location = {Paris, France},
series = {PVM/MPI'07}
}

@inproceedings{spec2017,
author = {Bucek, James and Lange, Klaus-Dieter and v. Kistowski, J\'{o}akim},
title = {{SPEC CPU2017: next-generation compute benchmark}},
year = {2018},
isbn = {9781450356299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185768.3185771},
doi = {10.1145/3185768.3185771},
abstract = {Description of the new features of the SPEC CPU2017 industry standard benchmark and its metric calculations.},
booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {41–-42},
numpages = {2},
keywords = {CPU, SPEC, compiler, performance},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{nvlink1,
  title={{Ultra-performance pascal GPU and NVLink interconnect}},
  author={Foley, Denis and Danskin, John},
  booktitle={IEEE Micro},
  year={2017},
  pages={7--17},
}

@article{nvlink1,
author = {Foley, Denis and Danskin, John},
title = {{Ultra-performance pascal GPU and NVLink interconnect}},
year = {2017},
issue_date = {March 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {37},
number = {2},
issn = {0272-1732},
url = {https://doi.org/10.1109/MM.2017.37},
doi = {10.1109/MM.2017.37},
abstract = {This article introduces Nvidia's high-performance Pascal GPU. GP100 features in-package high-bandwidth memory, support for efficient FP16 operations, unified memory, and instruction preemption, and incorporates Nvidia's NVLink I/O for high-bandwidth connections between GPUs and between GPUs and CPUs.},
journal = {IEEE Micro},
month = mar,
pages = {7–-17},
numpages = {11}
}

@ARTICLE{nvlink2,
  author={Choquette, Jack and Giroux, Olivier and Foley, Denis},
  journal={IEEE Micro},
  title={{Volta: performance and programmability}},
  year={2018},
  volume={38},
  number={2},
  pages={42--52},
  keywords={Graphics processing units;Central Processing Unit;Processor scheduling;Tensile stress;Bandwidth;Instruction sets;GPU;GV100;Volta;NVLink;deep learning;tensor core;Tesla;HPC;CUDA},
  doi={10.1109/MM.2018.022071134}}


@misc{intelmanual,
  title = {{Intel 64 and IA-32 architectures software developer manuals}},
  author = "{Intel}",
  howpublished = {\url{https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html}},
  note = {Accessed: 02-19-2025}
}

@misc{killundef,
  title = {{RFC: Killing undef and spreading poison}},
  author = "{LLVM Developers}",
  howpublished = {\url{https://lists.llvm.org/pipermail/llvm-dev/2016-December/107840.html}},
  note = {Accessed: 02-19-2025}
}

@misc{llvmmca,
  title = {{LLVM machine code analyzer}},
  author = "{LLVM Developers}",
  howpublished = {\url{https://llvm.org/docs/CommandGuide/llvm-mca.html}},
  note = {Accessed: 02-19-2025}
}

@misc{MemorySSA,
  title = {{MemorySSA}},
  author = "{{LLVM} Developers}",
  howpublished = {\url{https://llvm.org/docs/MemorySSA.html}},
  note = {Accessed: 02-19-2025}
}


@misc{loopinfo,
  title = {{LLVM loop terminology (and canonical forms)}},
  author = "{LLVM Developers}",
  howpublished = {\url{https://llvm.org/docs/LoopTerminology.html}},
  note = {Accessed: 02-19-2025}
}

@misc{hexagon,
  author = {{Qualcomm Technologies}},
  title = {{Hexagon DSP SDK}},
  howpublished = {\url{https://developer.qualcomm.com/software/hexagon-dsp-sdk}},
  note = {Accessed: 02-19-2025}
}

@misc{orc,
  title = {{ORC design and implementation}},
  author = "{LLVM Developers}",
  howpublished = {\url{https://llvm.org/docs/ORCv2.html}},
  note = {Accessed: 02-19-2025}
}

@misc{InstCombine,
  title = {{InstCombine contributor guide}},
  author = "{LLVM Developers}",
  howpublished = {\url{https://llvm.org/docs/InstCombineContributorGuide.html}},
  note = {Accessed: 02-19-2025}
}

@misc{llvmvectorizers,
  title = {{Auto-vectorization in LLVM}},
  author = "{LLVM Developers}",
  howpublished = {\url{https://llvm.org/docs/Vectorizers.html}},
  note = {Accessed: 02-19-2025}
}

@misc{tti,
  title = {{TargetTransformInfo class reference}},
  author = "{LLVM Developers}",
  howpublished = {\url{https://llvm.org/doxygen/classllvm\_1\_1TargetTransformInfo.html}},
  note = {Accessed: 02-19-2025}
}

@misc{neon,
  title = {{ARM NEON architecture}},
  author = "{ARM}",
  howpublished = {\url{https://developer.arm.com/Architectures/Neon}},
  note = {Accessed: 02-19-2025}
}

@misc{intelguide,
  title = {{Intel intrinsics guide}},
  author = "{Intel}",
  howpublished = {\url{https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html}},
  note = {Accessed: 02-19-2025}
}

@misc{cascadelake,
  title = {{Cascade Lake: Overview}},
  author = "{Intel}",
  howpublished = {\url{https://www.intel.com/content/www/us/en/products/platforms/details/cascade-lake.html}},
  note = {Accessed: 02-19-2025}
}

@misc{zen3,
  title = {{AMD Zen core architecture}},
  author = "{Advanced Micro Devices}",
  howpublished = {\url{https://www.amd.com/en/technologies/zen-core.html}},
  note = {Accessed: 02-19-2025}
}

@misc{mi50,
  title = {{AMD Radeon Instinct MI50 accelerator specifications}},
  author = "{Advanced Micro Devices}",
  howpublished = {\url{https://www.amd.com/en/support/downloads/drivers.html/accelerators/instinct/instinct-mi-series/instinct-mi50.html}},
  note = {Accessed: 02-19-2025}
}

@misc{rccl,
  title = {{ROCm communication collectives library}},
  author = "{Advanced Micro Devices}",
  howpublished = {\url{https://github.com/ROCmSoftwarePlatform/rccl}},
  note = {Accessed: 02-19-2025}
}

@misc{tpu,
  title = {{Google cloud tensor processing units}},
  author = "{Google}",
  howpublished = {\url{https://cloud.google.com/tpu}},
  note = {Accessed: 02-19-2025}
}

@misc{graphcore,
  title = {{Graphcore intelligence processing unit}},
  author = {Graphcore},
  howpublished = {\url{https://www.graphcore.ai/products/ipu}},
  note = {Accessed: 02-19-2025}
}

@misc{gloo,
  title = {{Gloo collective communications library}},
  author = {{Facebook}},
  howpublished = {\url{https://github.com/facebookincubator/gloo}},
  note = {Accessed: 02-19-2025}
}

@misc{ucx,
  title = {{Unified Communication X documentation}},
  author = {{Unified Communication X}},
  howpublished = {\url{https://openucx.org/documentation/}},
  note = {Accessed: 02-19-2025}
}

@misc{nccl,
  title = {{NVIDIA collective communications library}},
  author = {{NVIDIA}},
  howpublished = {\url{https://github.com/NVIDIA/nccl}},
  note = {Accessed: 02-19-2025}
}

@misc{dongarra2013mpi,
  title={{MPI: A message-passing interface standard version 3.0}},
  author={Dongarra, Jack and others},
  howpublished={\url{https://www.mpi-forum.org/docs/mpi-3.0/mpi30-report.pdf}},
  note = {Accessed: 02-19-2025}
}

@misc{Sands11,
  author = "Sands, Duncan",
  title = {{Super-optimizing LLVM IR}},
  howpublished = {\url{http://llvm.org/devmtg/2011-11/Sands_Super-optimizingLLVMIR.pdf}},
  note = {Accessed: 02-19-2025}
}
