\chapter{Background}
\label{chap:background}

\section {Translation Validation}

Translation validation checks if a program preserves its semantics
when translated from one representation to another.
%
This is particularly important in the context of compilers,
where multiple stages of translation are involved in transforming a
high-level programming language into machine code.
%
The correctness of the translation process is crucial to ensure that
the compiled program behaves as expected and that optimizations
performed by the compiler do not introduce errors.
%
Translation validation can be achieved using formal methods, such as
theorem proving, model checking, and symbolic execution.
%
This work focuses on translation validation in the context of
LLVM IR, specifically, for transformations that involve vector
instructions and SIMD operations, using the Alive2 Translation Validator.


\section {SMT Solving and the Z3 Theorem Prover}

Satisfiability Modulo Theories (SMT) solving is a technique used to
decide the satisfiability of logical formulas with respect to
background theories.
%
An SMT solver is a tool that can automatically determine whether a
given SMT formula is satisfiable or not, and if it is satisfiable,
provide a model that satisfies the formula.
%
SMT solvers are widely used in formal verification, program analysis,
and software testing.

This dissertation uses the Z3 theorem prover as a backend for program
synthesis and verification tasks.
%
The Z3 theorem prover is a state-of-the-art SMT solver developed by
Microsoft, which supports a wide range of theories, including
arithmetic, bit-vectors, arrays, and uninterpreted functions.
%

\section {Alive2 Translation Validator}

The primary goal of Alive2 is to ensure that the transformations made
by the LLVM compiler during optimization phases do not alter the
intended behavior of the original program.
%
Specifically, for a given pair of functions,
Alive2 checks if one function is a refinement of the other.
%
Here, We uses the word \emph{refinement}, rather than \emph{equivalence},
because compilers is allowed to remove non-determinism from the
program.
%
For example, an out-of-bound \texttt{extractelement} instruction can
be safely removed by the compiler, as the result of such an
instruction is undefined in LLVM IR.
%
In the absence of undefined behavior, refinement degrades to
simply equivalence.

Alive2 supports a wide range of LLVM operations, including integer
arithmetic, floating-point operations, memory operations, and vector
operations.
%
The chapter\ref{} of this desertation focuses on extending Alive2 to
support vector operations and floating-point operations, which
are essential for verifying optimizations related to SIMD and
floating-point instructions.

\section{Superoptimization}
\label{sec:superoptimization}


Superoptimization is a technique that aims to find
the optimal sequence of instructions for a particular task.
%
Unlike traditional optimization methods that rely on heuristics or
general rules to improve code efficiency, superoptimization
systematically searches the space of possible program transformations
to identify the better sequence of instructions.
%
This can result in significantly more efficient code, sometimes
surpassing what expert programmers or compilers can achieve.

The concept of superoptimization was first introduced by Alexia
Massalin in a 1987 paper, where she described a system for generating
the shortest possible sequence of machine instructions that performed
the same function as a given piece of assembly code. The
superoptimizer would test all possible combinations of instructions
and compare the output against the original function to ensure
correctness.

STOKE is a superoptimizer that uses stochastic search techniques to
explore the space of possible instruction sequences and find the most
efficient code for a given task. STOKE-FP is an extension of STOKE
that focuses on floating-point operations.

Souper is a superoptimizer that works on LLVM IR, the intermediate
representation used by the LLVM compiler infrastructure. Souper
extracts slices of LLVM instructions from a function and uses an SMT
solver to find optimized versions of these slices. It is designed to
work within the LLVM optimization pipeline and discover new
optimizations that traditional compilers might miss.

Chapter 4 of this dissertation presents Minotaur, a superoptimizer
that uses program synthesis to optimize LLVM IR code.

\section{Single Instruction, Multiple Data (SIMD) Parallelism}
\label{sec:simd}

SIMD stands for Single Instruction, Multiple Data, a type of parallel
computing architecture found within a CPU or GPU. It allows for the
execution of the same operation on multiple data points
simultaneously, making it highly effective for tasks that require the
same processing to be applied to large sets of data. Here's a detailed
look at SIMD and its key aspects:

SIMD exploits data-level parallelism by applying a single instruction
to multiple data points at once. This is different from multiple
instruction, multiple data (MIMD) architectures where different
processors execute different instructions on different data. By
processing multiple data points simultaneously, SIMD can significantly
speed up computations, especially in applications that involve large
arrays of data such as images, audio, and video streams.

SIMD operations are often carried out by vector processors or vector
units within a CPU. These processors have registers that can hold
multiple data elements and execute vectorized instructions on them.

Many CPU architectures include SIMD instruction set extensions that
enhance their ability to perform specific types of parallel
operations. Common examples include: Intel’s MMX and SSE (Streaming
SIMD Extensions): These extensions add new instructions and larger
registers to handle SIMD operations more efficiently. ARM’s NEON: This
is ARM’s equivalent of SIMD for its processor architectures, used
extensively in mobile devices. AVX (Advanced Vector Extensions) by
Intel: This offers enhanced performance and higher bit rates for
processing, suitable for scientific computations and high-performance
tasks.

\section{SIMD support in LLVM Optimization Pipeline}
\label{sec:llvm-vectors}

LLVM is a popular open-source compiler infrastructure that supports
various programming languages and architectures. It provides a
high-level intermediate representation (IR) that can be optimized and
compiled to machine code for different platforms. LLVM includes
support for vector operations, which allow programmers to work with
vectors of data efficiently. Here are some key aspects of vectors
support in LLVM:

LLVM provides robust support for vector operations, essential for
optimizing performance in applications that can benefit from SIMD
(Single Instruction, Multiple Data) capabilities. Her is a detailed
overview of how LLVM handles vector support:

\begin{itemize}

\item Vector types: LLVM defines vector types that represent arrays of
  elements of the same type. For example, a vector type might be
  defined as <4 x i32>, representing a vector of four 32-bit integers.
  These types can be used in LLVM IR to perform operations on vectors
  of data.

\item Vector operations: LLVM provides a set of vector operations that
  work on vector types. These operations include arithmetic operations,
  logical operations, shuffling, and other vector-specific
  instructions. Programmers can use these operations to write
  high-performance code that takes advantage of SIMD capabilities.
  A summary of the vector operations is listed in the next section.

\item Vectorization: LLVM has two main auto-vectorization passes, the
  Loop Vectorizer and the Superword Level Parallelism (SLP)
  Vectorizer.
  %
  The Loop Vectorizer optimizes loops by transforming scalar loop body
  into vectorized code, while the SLP Vectorizer optimizes
  straight-line code by combining multiple scalar operations into
  vector operations.
  %
  These vectorizers help improve performance by utilizing SIMD
  instructions, without requiring manual intervention.

\item Platform-independent peephole optimizations: The peephole
  optimization pass in LLVM, InstCombine, is designed in a way that
  the scalar optimizations can be applied to element-wise vector
  operations without any additional effort.
  %
  This is because the InstCombine matching and rewriting APIs are
  designed to be agnostic to the vector length, and the same
  optimization rules can be applied to both scalar and vector
  instructions.

\item Target-specific optimizations: LLVM can generate target-specific
  code that leverages the SIMD capabilities of the underlying
  hardware.
  %
  TTI (Target Transform Information) is an analysis pass of LLVM that
  provides information about the target architecture, including
  details about vectorization support.
  %
  The TTI pass allows LLVM middle-end to make optimization decisions
  based on the target architecture.


\item Intrinsic functions: LLVM provides intrinsic functions that
  allow optimization developers to directly access SIMD instructions
  provided by the target architecture.
  %
  These intrinsics provide a way
  to write low-level code that takes full advantage of the vector
  units available on the hardware.

\end{itemize}

\section {Vector Operations in LLVM}
\label{sec:llvm-vector-ops}

LLVM IR provides a rich set of vector operations that allow
programmers to work with vectors of data efficiently. Here is a
summary of some of the vector operations available in LLVM.

\begin{itemize}
  \item Arithmetic operations: LLVM supports vectorized arithmetic
  operations such as integer and floating point addition, subtraction,
  multiplication, and division on vectors.
  \item Logical operations: Vectorized logical operations like AND, OR
  and XOR can be performed on integer vectors.
  \item \texttt{insertelement} and \texttt{extractelement}: These
  operations allow programs to insert or extract elements from a
  vector at a specified index.
  \item \texttt{shufflevector}: The \texttt{shufflevector} instruction
  selects elements from two input vectors to create a new vector,
  controlled by a mask vector.
  \item Reductions: LLVM provides reduction operations like add, mul,
  min, and max that combine all elements of a vector into a single
  value.
  \item Predicated operations: LLVM supports predicated vector
  operations that conditionally execute based on a mask vector.
\end{itemize}


\section {Collective Communication Libraries}

A collective communication library is a software library that provides
high-level abstractions for collective communication operations in
parallel computing. Collective communication operations involve
multiple processes or threads coordinating to exchange data or
synchronize their operations. These libraries offer optimized
implementations of common collective communication patterns.
A collective communication library usually include following operations:

\begin{itemize}
    \item Broadcast: A process sends a message to all other processes.
    \item Scatter: A process sends different parts of an array to
    different processes.
    \item Gather: Processes send their data to a single process.
    \item Reduce: Processes combine their data using an operation such as
    addition.
    \item Allreduce: A combination of reduce and broadcast where all
    processes receive the result of the reduction.
    \item Allgather: A combination of gather and broadcast where all
    processes receive data from all other processes.
    \item Alltoall: Processes exchange data with all other processes.
\end{itemize}


There are several collective communication libraries available for
parallel computing, such as MPI (Message Passing Interface), NCCL
(NVIDIA Collective Communications Library), and RCCL (Radeon Open
Compute Collective Library). These libraries provide optimized
implementations of common collective communication operations and
enable efficient communication between parallel processes or threads.

The collective communication library is widely used in data-parallel
training of deep learning models, distributed computing, and
high-performance computing applications. By providing high-level
abstractions for collective communication operations, these libraries
simplify the development of parallel applications and enable efficient
communication between processes or threads.