\chapter{Background}
\label{chap:background}

\section {SMT Solving and the Z3 Theorem Prover}

Satisfiability Modulo Theories (SMT)~\cite{barrett2018satisfiability}
solving is a technique used to decide the satisfiability of logical
formulas with respect to background theories.
%
An SMT solver is a tool that can automatically determine whether a
given SMT formula is satisfiable or not, and if it is satisfiable,
provide a model that satisfies the formula.
%
SMT solvers are widely used in formal verification, program analysis,
and software testing.

This dissertation uses the Z3 theorem prover~\cite{z3} as a backend
for program synthesis and verification tasks.
%
The Z3 theorem prover is a state-of-the-art SMT solver developed by
Microsoft, which supports a wide range of theories, including
arithmetic, bit-vectors, arrays, and uninterpreted functions.

\section {Translation Validation}

Translation validation checks if a program preserves its semantics
when translated from one representation to another.
%
This is particularly important in the context of compilers,
where multiple stages of translation are involved in transforming a
high-level programming language into machine code.
%
The correctness of the translation process is crucial to ensure that
the compiled program behaves as expected, and that optimizations
performed by the compiler do not introduce errors.
%
Translation validation can be achieved using formal methods, such as
theorem proving, model checking, and symbolic execution.
%
This work focuses on translation validation in the context of LLVM
IR~\cite{LLVM:CGO04}, specifically for transformations that involve
vector instructions and SIMD operations, using the
Alive2~\cite{alive2} Translation Validator.

\section {Undefined Behavior in LLVM IR}


LLVM IR explicitly models undefined behavior using
two kinds of deferred undefined behavior values, \emph{poison} and
\emph{undef}, as well as immediate undefined behavior~\cite{taming}.

\begin{itemize}
%
\item The immediate undefined behavior is an operation that, when
executed, the program is in a corrupted state.
%
The immediate undefined behavior is triggered by operations such as
division by zero or dereferencing a null pointer.

\item A poison value is a special value that represents the result of
erroneous operation.
%
In order to allow aggressive optimizations, LLVM IR allows poison
values to propagate through the program if the operation does not
invoke immediate undefined behavior.
%
On the other hand, if a poison value is used as an operand that has
any values that trigger immediate undefined behavior, immediate
undefined behavior occurs.

\item An undef value represents an undefined bit-pattern, and it is
weaker than poison. The undef value is used to model uninitialized
values.
%
This dissertation does not study undef values, as they are in the
process of being deprecated by the LLVM community~\cite{killundef}.
\end{itemize}

\section {Alive2 Translation Validator}

The primary goal of Alive2 is to ensure that the transformations made
by the LLVM compiler during optimization phases do not alter the
intended behavior of the original program.
%
Specifically, for a given pair of functions,
Alive2 checks if one function is a refinement of the other.
%
Here, we use the word \emph{refinement}, rather than \emph{equivalence},
because compilers are allowed to remove non-determinism from the
program.
%
For example, an out-of-bound \texttt{extractelement} instruction can
be safely removed by the compiler, as the result of such an
instruction is poison in LLVM IR.
%
In the absence of undefined behavior, refinement degrades to
simply equivalence.

Alive2 supports a wide range of LLVM operations, including integer
arithmetic, floating-point operations, memory operations, and vector
operations.
%
Chapter~\ref{chap:extending-alive2} focuses on extending Alive2 to
support vector operations and floating-point operations, which are
essential for verifying optimizations related to SIMD and
floating-point instructions.

\section{Superoptimization}
\label{sec:superoptimization}


Unlike traditional optimization methods that rely on heuristics or
general rules to improve code efficiency, superoptimization is a
technique that looks at the code being compiled and uses a search
procedure and an equivalence checker to automatically discover better
code sequences.
%
This can result in significantly more efficient code, sometimes
surpassing what expert programmers or compilers can achieve.

The concept of superoptimization was first introduced by Alexia
Massalin in a 1987 paper~\cite{massalin}, where she described a system for generating
the shortest possible sequence of machine instructions that performed
the same function as a given piece of assembly code. The
superoptimizer would test all possible combinations of instructions
and compare the output against the original function to ensure
correctness.

STOKE~\cite{stoke} is a superoptimizer that uses stochastic search techniques to
explore the space of possible instruction sequences and find the most
efficient code for a given task. STOKE-FP~\cite{stoke-fp} is an extension of STOKE
that focuses on floating-point operations.

Souper~\cite{souper} is a superoptimizer that works on LLVM IR, the intermediate
representation used by the LLVM compiler infrastructure. Souper
extracts slices of LLVM instructions from a function and uses an SMT
solver to find optimized versions of these slices. It is designed to
work within the LLVM optimization pipeline and discover new
optimizations that traditional compilers might miss.

Chapter~\ref{chap:minotaur} of this dissertation presents Minotaur, a superoptimizer
that uses program synthesis to optimize LLVM IR code.

\section{Single Instruction, Multiple Data (SIMD) Parallelism}
\label{sec:simd}

SIMD stands for Single Instruction, Multiple Data, a type of parallel
computing architecture found within a CPU or GPU. SIMD exploits
data-level parallelism by applying a single instruction to multiple
data points at once.
%
By processing multiple data points simultaneously, SIMD can
significantly speed up computations, especially in applications that
involve large arrays of data such as images, audio, and video streams.

SIMD operations are often carried out by vector units within a CPU.
These processors have registers that can hold multiple data elements
and execute vectorized instructions.
%
Common examples of SIMD
architecture include Intel’s MMX, SSE, AVX, AVX2, and AVX-512
extensions, as well as ARM’s NEON SIMD ISA.


\section{SIMD support in LLVM Optimization Pipeline}
\label{sec:llvm-vectors}

LLVM~\cite{llvm} is a popular open-source compiler infrastructure that supports
various programming languages and architectures. It provides a
high-level intermediate representation (IR) that can be optimized and
compiled to machine code for different platforms. LLVM includes
support for vector operations, which allow programmers to work with
vectors of data efficiently. Here are some key aspects of vector
support in LLVM:

LLVM provides robust support for vector operations, essential for
optimizing performance in applications that can benefit from SIMD
(Single Instruction, Multiple Data) capabilities. Here is a detailed
overview of how LLVM handles vector support:

\begin{itemize}

\item Vector types: LLVM defines vector types that represent arrays of
  elements of the same type. For example, a vector type might be
  defined as $<$4 x i32$>$, representing a vector of four 32-bit integers.
  These types can be used in LLVM IR to perform operations on vectors
  of data.

\item Vector operations: LLVM provides a set of vector operations that
  work on vector types. These operations include arithmetic operations,
  logical operations, shuffling, and other vector-specific
  instructions. Programmers can use these operations to write
  high-performance code that takes advantage of SIMD capabilities.

\item Vectorization: LLVM has two auto-vectorization passes: the
  Loop Vectorizer and the Superword Level Parallelism (SLP)
  Vectorizer.
  %
  The Loop Vectorizer optimizes loops by transforming the scalar loop body
  into vectorized code, while the SLP Vectorizer optimizes
  straight-line code by combining multiple scalar operations into
  vector operations.
  %
  These vectorizers help improve performance by utilizing SIMD
  instructions, without requiring manual intervention.

\item Platform-independent peephole optimizations: The peephole
  optimization pass in LLVM, InstCombine, is designed in a way that
  the scalar optimizations can be applied to element-wise vector
  operations without any additional effort.
  %
  This is because the InstCombine matching and rewriting APIs are
  designed to be agnostic to the vector length, and the same
  optimization rules can be applied to both scalar and vector
  instructions.

\item Target-specific optimizations: LLVM can generate target-specific
  code that leverages the SIMD capabilities of the underlying
  hardware.
  %
  Target Transform Information (TTI) is an analysis pass of LLVM that
  provides information about the target architecture, including
  details about vectorization support.
  %
  The TTI pass allows the LLVM middle-end to make optimization decisions
  based on the target architecture.


\end{itemize}

LLVM IR provides a rich set of vector operations that allow
optimization passes to work with vectors. Here is a summary of vector
operations available in LLVM.

\begin{itemize}
  \item Arithmetic operations: LLVM supports vectorized arithmetic
  operations such as integer and floating point addition, subtraction,
  multiplication, and division on vectors.
  \item Logical operations: Vectorized logical operations like AND, OR
  and XOR can be performed on integer vectors.
  \item \texttt{insertelement} and \texttt{extractelement}: These
  operations allow programs to insert or extract elements from a
  vector at a specified index.
  \item \texttt{shufflevector}: The \texttt{shufflevector} instruction
  selects elements from two input vectors to create a new vector,
  controlled by a mask vector.
  \item Reductions: LLVM provides reduction operations like add, mul,
  min, and max that combine all elements of a vector into a single
  value.
  \item Predicated operations: LLVM supports predicated vector
  operations that conditionally execute based on a mask vector.
  \item Intrinsic functions: LLVM provides intrinsic functions that
  allow optimization developers to directly access SIMD instructions
  provided by the target architecture.
\end{itemize}


\section {Collective Communication Libraries}

A collective communication library is a software library that provides
high-level abstractions for collective communication operations in
parallel computing. Collective communication operations involve
multiple processes or threads coordinating to exchange data or
synchronize their operations. These libraries offer optimized
implementations of common collective communication patterns.
A collective communication library usually includes the following operations:

\begin{itemize}
    \item Broadcast: A process sends a message to all other processes.
    \item Scatter: A process sends different parts of an array to
    different processes.
    \item Gather: Processes send their data to a single process.
    \item Reduce: Processes combine their data using an operation such as
    addition.
    \item Allreduce: A combination of reduce and broadcast where all
    processes receive the result of the reduction.
    \item Allgather: A combination of gather and broadcast where all
    processes receive data from all other processes.
    \item Alltoall: Processes exchange data with all other processes.
\end{itemize}


There are several collective communication libraries available for
parallel computing, such as OpenMPI (Open Message Passing
Interface)~\cite{OpenMPI}, NCCL (NVIDIA Collective Communications
Library), and RCCL (ROCm Open Compute Collective Library).
%
These libraries provide optimized implementations of common collective
communication operations and enable efficient communication between
parallel processes or devices.
%
These collective communication libraries are widely used in data-parallel
training of deep learning models, distributed computing, and
high-performance computing applications.