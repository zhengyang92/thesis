\chapter{Background}
\label{chap:background}

\section {Translation Validation}

Translation validation is a technique used to ensure that a program
preserves its semantics when translated from one representation to
another.
%
This is particularly important in the context of compilers,
where multiple stages of translation are involved in transforming a
high-level programming language into machine code.
%
The correctness of the translation process is crucial to ensure that
the compiled program behaves as expected and that optimizations
performed by the compiler do not introduce errors.
%
Translation validation can be achieved using formal methods, such as
theorem proving, model checking, and symbolic execution.
%
This work focuses on translation validation in the context of
LLVM IR, specifically, for transformations that involve vector
instructions and SIMD operations, using the Alive2 Translation Validator.


\section {SMT Solving and the Z3 Theorem Prover}

Satisfiability Modulo Theories (SMT) solving is a technique used to
decide the satisfiability of logical formulas with respect to
background theories.
%
An SMT solver is a tool that can automatically
determine whether a given formula is satisfiable or not, and if it is
satisfiable, provide a model that satisfies the formula.
%
SMT solvers are widely used in formal verification, program analysis,
and software testing.

This dissertation uses the Z3 theorem prover as a backend for program
synthesis and verification tasks.
%
The Z3 theorem prover is a state-of-the-art SMT solver developed by
Microsoft, which supports a wide range of theories, including
arithmetic, bit-vectors, arrays, and uninterpreted functions.



\section {Alive2 Translation Validator}

Alive2 is a translation validator for LLVM IR that checks



\section{Superoptimization}
\label{sec:superoptimization}


Superoptimization is a technique in computer science aimed at finding
the optimal sequence of instructions for a particular task.
%
Unlike traditional optimization methods that rely on heuristics or
general rules to improve code efficiency, superoptimization
systematically searches the space of possible program transformations
to identify the better sequence of instructions.
%
This can result in significantly more efficient code, sometimes
surpassing what expert programmers or compilers can achieve.

The concept of superoptimization was first introduced by Alexia
Massalin in a 1987 paper, where she described a system for generating
the shortest possible sequence of machine instructions that performed
the same function as a given piece of assembly code. The
superoptimizer would test all possible combinations of instructions
and compare the output against the original function to ensure
correctness.

STOKE is a superoptimizer that uses stochastic search techniques to
explore the space of possible instruction sequences and find the most
efficient code for a given task. STOKE-FP is an extension of STOKE
that focuses on floating-point operations.

Souper is a superoptimizer that works on LLVM IR, the intermediate
representation used by the LLVM compiler infrastructure. Souper
extracts slices of LLVM instructions from a function and uses an SMT
solver to find optimized versions of these slices. It is designed to
work within the LLVM optimization pipeline and discover new
optimizations that traditional compilers might miss.

\section{Single Instruction, Multiple Data (SIMD) Parallelism}
\label{sec:simd}

SIMD stands for Single Instruction, Multiple Data, a type of parallel
computing architecture found within a CPU or GPU. It allows for the
execution of the same operation on multiple data points
simultaneously, making it highly effective for tasks that require the
same processing to be applied to large sets of data. Here's a detailed
look at SIMD and its key aspects:

SIMD exploits data-level parallelism by applying a single instruction
to multiple data points at once. This is different from multiple
instruction, multiple data (MIMD) architectures where different
processors execute different instructions on different data. By
processing multiple data points simultaneously, SIMD can significantly
speed up computations, especially in applications that involve large
arrays of data such as images, audio, and video streams.

SIMD operations are often carried out by vector processors or vector
units within a CPU. These processors have registers that can hold
multiple data elements and execute vectorized instructions on them.

Many CPU architectures include SIMD instruction set extensions that
enhance their ability to perform specific types of parallel
operations. Common examples include: Intel’s MMX and SSE (Streaming
SIMD Extensions): These extensions add new instructions and larger
registers to handle SIMD operations more efficiently. ARM’s NEON: This
is ARM’s equivalent of SIMD for its processor architectures, used
extensively in mobile devices. AVX (Advanced Vector Extensions) by
Intel: This offers enhanced performance and higher bit rates for
processing, suitable for scientific computations and high-performance
tasks.

\section{SIMD support in LLVM Optimization Pipeline}
\label{sec:llvm-vectors}

LLVM is a popular open-source compiler infrastructure that supports
various programming languages and architectures. It provides a
high-level intermediate representation (IR) that can be optimized and
compiled to machine code for different platforms. LLVM includes
support for vector operations, which allow programmers to work with
vectors of data efficiently. Here are some key aspects of vectors
support in LLVM:

LLVM provides robust support for vector operations, essential for
optimizing performance in applications that can benefit from SIMD
(Single Instruction, Multiple Data) capabilities. Here’s a detailed
overview of how LLVM handles vector support:

\begin{itemize}

\item Vector types: LLVM defines vector types that represent arrays of
  elements of the same type. For example, a vector type might be
  defined as <4 x i32>, representing a vector of four 32-bit integers.
  These types can be used in LLVM IR to perform operations on vectors
  of data.

\item Vector operations: LLVM provides a set of vector operations that
  work on vector types. These operations include arithmetic operations,
  logical operations, shuffling, and other vector-specific
  instructions. Programmers can use these operations to write
  high-performance code that takes advantage of SIMD capabilities.
  A summary of the vector operations is listed in the next section.

\item Vectorization: LLVM has two main auto-vectorization passes, the
  Loop Vectorizer and the Superword Level Parallelism (SLP)
  Vectorizer.
  %
  The Loop Vectorizer optimizes loops by transforming scalar loop body
  into vectorized code, while the SLP Vectorizer optimizes
  straight-line code by combining multiple scalar operations into
  vector operations.
  %
  These vectorizers help improve performance by utilizing SIMD
  instructions, without requiring manual intervention.

\item Platform-independent peephole optimizations: The peephole
  optimization pass in LLVM, InstCombine, is designed in a way that
  the scalar optimizations can be applied to element-wise vector
  operations without any additional effort.
  %
  This is because the InstCombine matching and rewriting APIs are
  designed to be agnostic to the vector length, and the same
  optimization rules can be applied to both scalar and vector
  instructions.

\item Target-specific optimizations: LLVM can generate target-specific
  code that leverages the SIMD capabilities of the underlying
  hardware.
  %
  TTI (Target Transform Information) is an analysis pass of LLVM that
  provides information about the target architecture, including
  details about vectorization support.
  %
  The TTI pass allows LLVM middle-end to make optimization decisions
  based on the target architecture.


\item Intrinsic functions: LLVM provides intrinsic functions that
  allow optimization developers to directly access SIMD instructions
  provided by the target architecture.
  %
  These intrinsics provide a way
  to write low-level code that takes full advantage of the vector
  units available on the hardware.

\end{itemize}

\section {Collective Communication Libraries}

A collective communication library is a software library that provides
high-level abstractions for collective communication operations in
parallel computing. Collective communication operations involve
multiple processes or threads coordinating to exchange data or
synchronize their operations. These libraries offer optimized
implementations of common collective communication patterns.
A collective communication library usually include following operations:

\begin{itemize}
    \item Broadcast: A process sends a message to all other processes.
    \item Scatter: A process sends different parts of an array to
    different processes.
    \item Gather: Processes send their data to a single process.
    \item Reduce: Processes combine their data using an operation such as
    addition.
    \item Allreduce: A combination of reduce and broadcast where all
    processes receive the result of the reduction.
    \item Allgather: A combination of gather and broadcast where all
    processes receive data from all other processes.
    \item Alltoall: Processes exchange data with all other processes.
\end{itemize}


There are several collective communication libraries available for
parallel computing, such as MPI (Message Passing Interface), NCCL
(NVIDIA Collective Communications Library), and RCCL (Radeon Open
Compute Collective Library). These libraries provide optimized
implementations of common collective communication operations and
enable efficient communication between parallel processes or threads.

The collective communication library is widely used in data-parallel
training of deep learning models, distributed computing, and
high-performance computing applications. By providing high-level
abstractions for collective communication operations, these libraries
simplify the development of parallel applications and enable efficient
communication between processes or threads.